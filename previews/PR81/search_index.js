var documenterSearchIndex = {"docs":
[{"location":"examples/svesm/#SVESM-1","page":"SVESM","title":"SVESM","text":"","category":"section"},{"location":"examples/svesm/#","page":"SVESM","title":"SVESM","text":"Leveraging the similarities between kernel methods and Reservoir Computing, the paper [1] introduced the concept of Support Vector Echo State Machines (SVESMs). Using the package LIBSVM the SVESMs are implemented in ReservoirComputing.jl. We will give an example of usage introducing the direct predict function as well. The goal is again the prediction of the Lorenz system.","category":"page"},{"location":"examples/svesm/#","page":"SVESM","title":"SVESM","text":"using ParameterizedFunctions, OrdinaryDiffEq\n\n#lorenz system parameters\nu0 = [1.0,0.0,0.0]                       \ntspan = (0.0,1000.0)                      \np = [10.0,28.0,8/3]\n\n#define lorenz system\nfunction lorenz(du,u,p,t)\n    du[1] = p[1]*(u[2]-u[1])\n    du[2] = u[1]*(p[2]-u[3]) - u[2]\n    du[3] = u[1]*u[2] - p[3]*u[3]\nend\n\n#solve system\nprob = ODEProblem(lorenz, u0, tspan, p)  \nsol = solve(prob, ABM54(), dt=0.02)   \nv = sol.u\ndata = Matrix(hcat(v...))\n\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#get data\ntrain = data[:, shift:shift+train_len-1]\ntest = data[:, shift+train_len:shift+train_len+predict_len-1]","category":"page"},{"location":"examples/svesm/#References-1","page":"SVESM","title":"References","text":"","category":"section"},{"location":"examples/svesm/#","page":"SVESM","title":"SVESM","text":"[1]: Shi, Zhiwei, and Min Han. \"Support vector echo-state machine for chaotic time-series prediction.\" IEEE Transactions on Neural Networks 18.2 (2007): 359-372.","category":"page"},{"location":"examples/dafesn/#Double-Activation-Function-ESN-1","page":"Double Activation Function ESN","title":"Double Activation Function ESN","text":"","category":"section"},{"location":"examples/dafesn/#","page":"Double Activation Function ESN","title":"Double Activation Function ESN","text":"One of the different variations of the ESN implemented in ReservoirComputing is the Double Activation Function ESN (DASFESN) [1]. The constructs are implemented in the same way as the ESN ones and the training is done with the same ESNtrain function. The only differences are the number of parameters and the predict function. Let's use the same Lorenz system example as always to see how to use this model.","category":"page"},{"location":"examples/dafesn/#","page":"Double Activation Function ESN","title":"Double Activation Function ESN","text":"using ParameterizedFunctions, OrdinaryDiffEq\n\n#lorenz system parameters\nu0 = [1.0,0.0,0.0]                       \ntspan = (0.0,1000.0)                      \np = [10.0,28.0,8/3]\n\n#define lorenz system\nfunction lorenz(du,u,p,t)\n    du[1] = p[1]*(u[2]-u[1])\n    du[2] = u[1]*(p[2]-u[3]) - u[2]\n    du[3] = u[1]*u[2] - p[3]*u[3]\nend\n\n#solve system\nprob = ODEProblem(lorenz, u0, tspan, p)  \nsol = solve(prob, ABM54(), dt=0.02)   \nv = sol.u\ndata = Matrix(hcat(v...))\n\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#get data\ntrain = data[:, shift:shift+train_len-1]\ntest = data[:, shift+train_len:shift+train_len+predict_len-1]","category":"page"},{"location":"examples/dafesn/#","page":"Double Activation Function ESN","title":"Double Activation Function ESN","text":"Now we can define the parameters as before: this time we will have two activation functions instead of one and a linear coefficient for each function defined as first_lambda and second_lambda. For the second activation function we will use the sigmoid function, which can be either user-defined or borrowed from another package. We will use the implementation of NNlib.","category":"page"},{"location":"examples/dafesn/#","page":"Double Activation Function ESN","title":"Double Activation Function ESN","text":"using ReservoirComputing, NNlib\n#model parameters\napprox_res_size = 300\nradius = 1.2\ndegree = 6\nsigma = 0.1\nbeta = 0.0\nalpha = 1.0\nnla_type = NLAT2()\nextended_states = false\n\nfirst_lambda = 0.8\nsecond_lambda = 0.4\nfirst_activation = tanh\nsecond_activation = σ\n\n#create echo state network  \nRandom.seed!(42) #fixed seed for reproducibility\nesn = dafESN(approx_res_size,\n    train,\n    degree,\n    radius,\n    first_lambda,\n    second_lambda,\n    first_activation = first_activation,\n    second_activation = second_activation,\n    sigma = sigma,\n    alpha = alpha,\n    nla_type = nla_type,\n    extended_states = extended_states)","category":"page"},{"location":"examples/dafesn/#","page":"Double Activation Function ESN","title":"Double Activation Function ESN","text":"plot(transpose(output),layout=(3,1), label=\"predicted\")\nplot!(transpose(test),layout=(3,1), label=\"actual\")","category":"page"},{"location":"examples/dafesn/#","page":"Double Activation Function ESN","title":"Double Activation Function ESN","text":"(Image: dafesnfixedseed)","category":"page"},{"location":"examples/dafesn/#","page":"Double Activation Function ESN","title":"Double Activation Function ESN","text":"In this example we used the standard constructor but the user is free to define the input layer and reservoir in the same way we had showed before. Of course, one could also train the DAFESN using one of the already illustrated linear models.","category":"page"},{"location":"examples/dafesn/#References-1","page":"Double Activation Function ESN","title":"References","text":"","category":"section"},{"location":"examples/dafesn/#","page":"Double Activation Function ESN","title":"Double Activation Function ESN","text":"[1]: Lun, Shu-Xian, et al. \"A novel model of leaky integrator echo state network for time-series prediction.\" Neurocomputing 159 (2015): 58-66.","category":"page"},{"location":"user/models/#Models-1","page":"Models","title":"Models","text":"","category":"section"},{"location":"user/models/#Echo-State-Network-1","page":"Models","title":"Echo State Network","text":"","category":"section"},{"location":"user/models/#","page":"Models","title":"Models","text":"Constructor","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"    ESN","category":"page"},{"location":"user/models/#ReservoirComputing.ESN","page":"Models","title":"ReservoirComputing.ESN","text":"ESN(W::AbstractArray{T}, train_data::AbstractArray{T}, W_in::AbstractArray{T}\n[, activation::Any, alpha::T, nla_type::NonLinearAlgorithm, extended_states::Bool])\n\nBuild an ESN struct given the input and reservoir matrices.\n\n\n\n\n\n","category":"type"},{"location":"user/models/#","page":"Models","title":"Models","text":"Train","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"    ESNtrain","category":"page"},{"location":"user/models/#ReservoirComputing.ESNtrain","page":"Models","title":"ReservoirComputing.ESNtrain","text":"ESNtrain(esn::AbstractReservoirComputer, beta::Float64[, train_data])\n\nReturn the trained output layer using Ridge Regression.\n\n\n\n\n\nESNtrain(lm::LinearModel, esn::AbstractReservoirComputer[, train_data])\n\nReturn the trained output layer using an MLJLinearModels method built into a LinearSolver struct.\n\n\n\n\n\n","category":"function"},{"location":"user/models/#","page":"Models","title":"Models","text":"General function for training, used for all the AbstractReservoirComputers types with the exception of RMM which has the training method inside the constructor.","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"Predict","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"    ESNpredict\n    ESNpredict_h_steps","category":"page"},{"location":"user/models/#ReservoirComputing.ESNpredict","page":"Models","title":"ReservoirComputing.ESNpredict","text":"ESNpredict(esn::AbstractLeakyESN, predict_len::Int, W_out::AbstractArray{Float64})\n\nReturn the prediction for a given length of the constructed ESN struct.\n\n\n\n\n\n","category":"function"},{"location":"user/models/#ReservoirComputing.ESNpredict_h_steps","page":"Models","title":"ReservoirComputing.ESNpredict_h_steps","text":"ESNpredict_h_steps(esn::AbstractLeakyESN, predict_len::Int, h_steps::Int,\ntest_data::AbstractArray{Float64}, W_out::AbstractArray{Float64})\n\nReturn the prediction for h steps ahead of the constructed ESN struct.\n\n\n\n\n\n","category":"function"},{"location":"user/models/#","page":"Models","title":"Models","text":"For a full list of training and prediction methods for ESNs please refer to the User Guide ESN mods.","category":"page"},{"location":"user/models/#Double-Activation-Function-Echo-State-Network-1","page":"Models","title":"Double Activation Function Echo State Network","text":"","category":"section"},{"location":"user/models/#","page":"Models","title":"Models","text":"Constructor","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"    dafESN","category":"page"},{"location":"user/models/#ReservoirComputing.dafESN","page":"Models","title":"ReservoirComputing.dafESN","text":"dafESN(W::AbstractArray{T}, train_data::AbstractArray{T}, first_lambda::T,\nsecond_lambda::T, W_in::AbstractArray{T} [, first_activation::Any,\nsecond_activation::Any, alpha::T, nla_type::NonLinearAlgorithm, extended_states::Bool])\n\nBuild a double activation function ESN struct given the input and reservoir matrices, as described in [1].\n\n[1]: Lun, Shu-Xian, et al. \"A novel model of leaky integrator echo state network for time-series prediction.\" Neurocomputing 159 (2015): 58-66.\n\n\n\n\n\n","category":"type"},{"location":"user/models/#","page":"Models","title":"Models","text":"Predict","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"    dafESNpredict\n    dafESNpredict_h_steps","category":"page"},{"location":"user/models/#ReservoirComputing.dafESNpredict","page":"Models","title":"ReservoirComputing.dafESNpredict","text":"dafESNpredict(esn::AbstractLeakyESN, predict_len::Int, W_out::AbstractArray{Float64})\n\nReturn the prediction for a given length of the constructed dafESN struct.\n\n\n\n\n\n","category":"function"},{"location":"user/models/#ReservoirComputing.dafESNpredict_h_steps","page":"Models","title":"ReservoirComputing.dafESNpredict_h_steps","text":"dafESNpredict_h_steps(esn::AbstractLeakyESN, predict_len::Int, h_steps::Int,\ntest_data::AbstractArray{Float64}, W_out::AbstractArray{Float64})\n\nReturn the prediction for h steps ahead of the constructed ESN struct.\n\n\n\n\n\n","category":"function"},{"location":"user/models/#Reservoir-Computing-with-Cellular-Automata-1","page":"Models","title":"Reservoir Computing with Cellular Automata","text":"","category":"section"},{"location":"user/models/#","page":"Models","title":"Models","text":"Constructors","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"    RECA_discrete\n    RECA_TwoDim","category":"page"},{"location":"user/models/#ReservoirComputing.RECA_discrete","page":"Models","title":"ReservoirComputing.RECA_discrete","text":"RECA_discrete(train_data::AbstractArray{Int}, rule::Int, generations::Int, \nexpansion_size::Int, permutations::Int [, nla_type])\n\nCreate a RECA struct as described in [1] and [2].\n\n[1] Yilmaz, Ozgur. “Reservoir computing using cellular automata.” arXiv preprint arXiv:1410.0162 (2014). [2] Nichele, Stefano, and Andreas Molund. “Deep reservoir computing using cellular automata.” arXiv preprint arXiv:1703.02806 (2017).\n\n\n\n\n\n","category":"type"},{"location":"user/models/#ReservoirComputing.RECA_TwoDim","page":"Models","title":"ReservoirComputing.RECA_TwoDim","text":"RECA_TwoDim(train_data, res_size, generations, permutations [, nla_type])\n\nCreate the RECA_TwoDim struct for two-dimensional Cellular Automata Reservoir Computing, as described in [1].\n\n[1] Yilmaz, Ozgur. “Reservoir computing using cellular automata.” arXiv preprint arXiv:1410.0162 (2014).\n\n\n\n\n\n","category":"type"},{"location":"user/models/#","page":"Models","title":"Models","text":"Predict","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"    RECAdirect_predict_discrete\n    RECATDdirect_predict_discrete\n    RECATD_predict_discrete","category":"page"},{"location":"user/models/#ReservoirComputing.RECAdirect_predict_discrete","page":"Models","title":"ReservoirComputing.RECAdirect_predict_discrete","text":"RECAdirect_predict_discrete(reca::AbstractReca, W_out::AbstractArray{Float64}, test_data::AbstractArray{Int})\n\nGiven the input data return the corresponding predicted output, as described in [1].\n\n[1] Yilmaz, Ozgur. “Reservoir computing using cellular automata.” arXiv preprint arXiv:1410.0162 (2014).\n\n\n\n\n\n","category":"function"},{"location":"user/models/#ReservoirComputing.RECATDdirect_predict_discrete","page":"Models","title":"ReservoirComputing.RECATDdirect_predict_discrete","text":"RECATDdirect_predict_discrete(reca::AbstractReca, W_out::AbstractArray{Float64}, test_data::AbstractArray{Int})\n\nGiven the input data return the corresponding predicted output, as described in [1].\n\n[1] Yilmaz, Ozgur. “Reservoir computing using cellular automata.” arXiv preprint arXiv:1410.0162 (2014).\n\n\n\n\n\n","category":"function"},{"location":"user/models/#ReservoirComputing.RECATD_predict_discrete","page":"Models","title":"ReservoirComputing.RECATD_predict_discrete","text":"RECATD_predict_discrete(reca, predict_len::Int, W_out::AbstractArray{Float64})\n\nReturn the prediction for a given length of the constructed RECA_TwoDim struct.\n\n\n\n\n\n","category":"function"},{"location":"user/models/#Reservoir-Memory-Machine-1","page":"Models","title":"Reservoir Memory Machine","text":"","category":"section"},{"location":"user/models/#","page":"Models","title":"Models","text":"Constructor","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"    RMM","category":"page"},{"location":"user/models/#ReservoirComputing.RMM","page":"Models","title":"ReservoirComputing.RMM","text":"RMM(W::AbstractArray{T}, in_data::AbstractArray{T}, out_data::AbstractArray{T}, W_in::AbstractArray{T},\nmemory_size::Int, beta::Float64 [, activation::Any, alpha::T, nla_type::NonLinearAlgorithm, extended_states::Bool])\n\nCreate a Reservoir Memory Machine struct, as described in [1].\n\n[1] Paaßen, Benjamin, and Alexander Schulz. \"Reservoir memory machines.\" arXiv preprint arXiv:2003.04793 (2020).\n\n\n\n\n\n","category":"type"},{"location":"user/models/#","page":"Models","title":"Models","text":"Predict","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"    RMMdirect_predict","category":"page"},{"location":"user/models/#ReservoirComputing.RMMdirect_predict","page":"Models","title":"ReservoirComputing.RMMdirect_predict","text":"RMMdirect_predict(rmm::AbstractReservoirMemoryMachine, input)\n\nGiven the input data return the corresponding predicted output, as described in [1].\n\n[1] Paaßen, Benjamin, and Alexander Schulz. \"Reservoir memory machines.\" arXiv preprint arXiv:2003.04793 (2020).\n\n\n\n\n\n","category":"function"},{"location":"user/models/#Gated-Recurrent-Unit-ESN-1","page":"Models","title":"Gated Recurrent Unit ESN","text":"","category":"section"},{"location":"user/models/#","page":"Models","title":"Models","text":"Constructor","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"    GRUESN","category":"page"},{"location":"user/models/#ReservoirComputing.GRUESN","page":"Models","title":"ReservoirComputing.GRUESN","text":"GRUESN(W::AbstractArray{T}, train_data::AbstractArray{T}, W_in::AbstractArray{T} [, gates_weight::T, activation::Any, alpha::T, nla_type::NonLinearAlgorithm, extended_states::Bool])\n\nReturn a Gated Recurrent Unit [1] ESN struct.\n\n[1] Cho, Kyunghyun, et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014).\n\n\n\n\n\n","category":"type"},{"location":"user/models/#","page":"Models","title":"Models","text":"Predict","category":"page"},{"location":"user/models/#","page":"Models","title":"Models","text":"    GRUESNpredict","category":"page"},{"location":"user/models/#ReservoirComputing.GRUESNpredict","page":"Models","title":"ReservoirComputing.GRUESNpredict","text":"GRUESNpredict(esn::AbstractGRUESN, predict_len::Int, W_out::AbstractArray{Float64})\n\nReturn the prediction for a given length of the constructed GRUESN.\n\n\n\n\n\n","category":"function"},{"location":"user/nla/#Non-Linear-Algorithm-1","page":"Non Linear Algorithm","title":"Non Linear Algorithm","text":"","category":"section"},{"location":"user/nla/#","page":"Non Linear Algorithm","title":"Non Linear Algorithm","text":"    NLADefault\n    NLAT1\n    NLAT2\n    NLAT3","category":"page"},{"location":"user/nla/#ReservoirComputing.NLADefault","page":"Non Linear Algorithm","title":"ReservoirComputing.NLADefault","text":"NLADefault()\n\nReturn the array untouched, default option.\n\n\n\n\n\n","category":"type"},{"location":"user/nla/#ReservoirComputing.NLAT1","page":"Non Linear Algorithm","title":"ReservoirComputing.NLAT1","text":"NLAT1()\n\nApply the $ \\text{T}_1 $ transformation algorithm, as defined in [1] and [2].\n\n[1] Chattopadhyay, Ashesh, et al. \"Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods: Reservoir computing, ANN, and RNN-LSTM.\" (2019).\n\n[2] Pathak, Jaideep, et al. \"Model-free prediction of large spatiotemporally chaotic systems from data: A reservoir computing approach.\" Physical review letters 120.2 (2018): 024102.\n\n\n\n\n\n","category":"type"},{"location":"user/nla/#ReservoirComputing.NLAT2","page":"Non Linear Algorithm","title":"ReservoirComputing.NLAT2","text":"NLAT2()\n\nApply the $ \\text{T}_2 $ transformation algorithm, as defined in [1].\n\n[1] Chattopadhyay, Ashesh, et al. \"Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods: Reservoir computing, ANN, and RNN-LSTM.\" (2019).\n\n\n\n\n\n","category":"type"},{"location":"user/nla/#ReservoirComputing.NLAT3","page":"Non Linear Algorithm","title":"ReservoirComputing.NLAT3","text":"NLAT3()\n\nApply the $ \\text{T}_3 $ transformation algorithm, as defined in [1].\n\n[1] Chattopadhyay, Ashesh, et al. \"Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods: Reservoir computing, ANN, and RNN-LSTM.\" (2019).\n\n\n\n\n\n","category":"type"},{"location":"examples/esgp/#ESGP-1","page":"ESGP","title":"ESGP","text":"","category":"section"},{"location":"examples/esgp/#","page":"ESGP","title":"ESGP","text":"The linear nature of the ESN training allows for a solution obtained by Gaussian Regression. This was the main idea behind the paper [1], which details the implementation of the Echo State Gaussian Processes that are present in ReservoirComputing. Using the same example as before, prediction of the Lorenz system, we are going to show how to use this specific model.","category":"page"},{"location":"examples/esgp/#","page":"ESGP","title":"ESGP","text":"using ParameterizedFunctions, OrdinaryDiffEq\n\n#lorenz system parameters\nu0 = [1.0,0.0,0.0]                       \ntspan = (0.0,1000.0)                      \np = [10.0,28.0,8/3]\n\n#define lorenz system\nfunction lorenz(du,u,p,t)\n    du[1] = p[1]*(u[2]-u[1])\n    du[2] = u[1]*(p[2]-u[3]) - u[2]\n    du[3] = u[1]*u[2] - p[3]*u[3]\nend\n\n#solve system\nprob = ODEProblem(lorenz, u0, tspan, p)  \nsol = solve(prob, ABM54(), dt=0.02)   \nv = sol.u\ndata = Matrix(hcat(v...))\n\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#get data\ntrain = data[:, shift:shift+train_len-1]\ntest = data[:, shift+train_len:shift+train_len+predict_len-1]","category":"page"},{"location":"examples/esgp/#","page":"ESGP","title":"ESGP","text":"Now we can define the parameters and create the ESN in the usual way:","category":"page"},{"location":"examples/esgp/#","page":"ESGP","title":"ESGP","text":"using ReservoirComputing\n#model parameters\napprox_res_size = 300\nradius = 1.2\nactivation = tanh\ndegree = 6\nsigma = 0.1\nbeta = 0.0\nalpha = 1.0\nnla_type = NLADefault()\nextended_states = true\n\n#create echo state network  \nRandom.seed!(42) #fixed seed for reproducibility\nesn = ESN(approx_res_size,\n    train,\n    degree,\n    radius,\n    activation = activation,\n    sigma = sigma,\n    alpha = alpha,\n    nla_type = nla_type,\n    extended_states = extended_states)","category":"page"},{"location":"examples/esgp/#","page":"ESGP","title":"ESGP","text":"Using the package GaussianProcesses we were able to implement a training and a predict function for the ESN. In order to use them, we need to import the package.","category":"page"},{"location":"examples/esgp/#","page":"ESGP","title":"ESGP","text":"using GaussianProcesses\nmean = MeanZero()\nkernel = SE(1.0, 1.0)\ngp = ESGPtrain(esn, mean, kernel, lognoise = -2.0, optimize = false);\noutput, sigmas = ESGPpredict(esn, predict_len, gp)","category":"page"},{"location":"examples/esgp/#","page":"ESGP","title":"ESGP","text":"plot(transpose(output),layout=(3,1), label=\"predicted\")\nplot!(transpose(test),layout=(3,1), label=\"actual\")","category":"page"},{"location":"examples/esgp/#","page":"ESGP","title":"ESGP","text":"(Image: esgpfixedseed)","category":"page"},{"location":"examples/esgp/#","page":"ESGP","title":"ESGP","text":"Since the implementation of this model is based on an external package, the user is free to choose a different mean or kernel, as well as use different input layers and reservoirs (as previously defined).","category":"page"},{"location":"examples/esgp/#References-1","page":"ESGP","title":"References","text":"","category":"section"},{"location":"examples/esgp/#","page":"ESGP","title":"ESGP","text":"[1]: Chatzis, Sotirios P., and Yiannis Demiris. \"Echo state Gaussian process.\" IEEE Transactions on Neural Networks 22.9 (2011): 1435-1445.","category":"page"},{"location":"examples/esn/#Basics-1","page":"Basics","title":"Basics","text":"","category":"section"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"The following is a basic example that will introduce the reader to the function and structs of the ReservoirComputing library.","category":"page"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"The goal for this example is to predict the Lorenz system, so first we need to obtain the data.","category":"page"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"using ParameterizedFunctions, OrdinaryDiffEq\n\n#lorenz system parameters\nu0 = [1.0,0.0,0.0]                       \ntspan = (0.0,1000.0)                      \np = [10.0,28.0,8/3]\n\n#define lorenz system\nfunction lorenz(du,u,p,t)\n    du[1] = p[1]*(u[2]-u[1])\n    du[2] = u[1]*(p[2]-u[3]) - u[2]\n    du[3] = u[1]*u[2] - p[3]*u[3]\nend\n\n#solve system\nprob = ODEProblem(lorenz, u0, tspan, p)  \nsol = solve(prob, ABM54(), dt=0.02)   \nv = sol.u\ndata = Matrix(hcat(v...))","category":"page"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"Now that we have the data, we can create two datasets, one for the training and another to test the results obtained:","category":"page"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"shift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#get data\ntrain = data[:, shift:shift+train_len-1]\ntest = data[:, shift+train_len:shift+train_len+predict_len-1]","category":"page"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"It is always a good idea to add a shift in order to wash out any initial transient. Having the data, we can proceed to creating the ESN for the prediction:","category":"page"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"using ReservoirComputing\n#model parameters\napprox_res_size = 300 #size of the reservoir\nradius = 1.2 #desired spectral radius\nactivation = tanh #neuron activation function\ndegree = 6 #degree of connectivity of the reservoir\nsigma = 0.1 # input weight scaling\nbeta = 0.0 #ridge\nalpha = 1.0 #leaky coefficient\nnla_type = NLAT2() #non linear algorithm for the states\nextended_states = false # if true extends the states with the input\n\n#create echo state network  \nesn = ESN(approx_res_size,\n    train,\n    degree,\n    radius,\n    activation = activation,\n    sigma = sigma,\n    alpha = alpha,\n    nla_type = nla_type,\n    extended_states = extended_states)","category":"page"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"Since this is an introductory example, we wanted to show all possible parameters, even though the same values that we defined are the default ones of the ESN constructor. For the training and the prediction we just need the following two lines:","category":"page"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"W_out = ESNtrain(esn, beta)\noutput = ESNpredict(esn, predict_len, W_out) #applies standard ridge regression for the training","category":"page"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"Now, if we want to check the results, we can plot the output and the test dataset:","category":"page"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"using Plots\nplot(transpose(output),layout=(3,1), label=\"predicted\")\nplot!(transpose(test),layout=(3,1), label=\"actual\")","category":"page"},{"location":"examples/esn/#","page":"Basics","title":"Basics","text":"(Image: lorenz_coord)","category":"page"},{"location":"examples/linear/#Using-different-linear-methods-1","page":"Using different linear methods","title":"Using different linear methods","text":"","category":"section"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"The standard implementation of ESNtrain uses Ridge Regression as the method of choice for the training of the ESN but there are other linear methods available. Leveraging MLJLinearModels ReservoirComputing gives the possibility to train ESNs using a vast range of linear models. Using the same task as before, predicting the Lorenz system, we will illustrate how to use ESNtrain with Lasso, Elastic Net, and regression with Huber loss function.","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"First, we obtain the data in the usual way:","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"using ParameterizedFunctions, OrdinaryDiffEq\n\n#lorenz system parameters\nu0 = [1.0,0.0,0.0]                       \ntspan = (0.0,1000.0)                      \np = [10.0,28.0,8/3]\n\n#define lorenz system\nfunction lorenz(du,u,p,t)\n    du[1] = p[1]*(u[2]-u[1])\n    du[2] = u[1]*(p[2]-u[3]) - u[2]\n    du[3] = u[1]*u[2] - p[3]*u[3]\nend\n\n#solve system\nprob = ODEProblem(lorenz, u0, tspan, p)  \nsol = solve(prob, ABM54(), dt=0.02)   \nv = sol.u\ndata = Matrix(hcat(v...))\n\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#get data\ntrain = data[:, shift:shift+train_len-1]\ntest = data[:, shift+train_len:shift+train_len+predict_len-1]","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"And we can also use the same parameters:","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"using ReservoirComputing\n#model parameters\napprox_res_size = 300\nradius = 1.2\nactivation = tanh\ndegree = 6\nsigma = 0.1\nalpha = 1.0\nnla_type = NLAT2()\nextended_states = false\n\n#create echo state network  \nesn = ESN(approx_res_size,\n    train,\n    degree,\n    radius,\n    activation = activation,\n    sigma = sigma,\n    alpha = alpha,\n    nla_type = nla_type,\n    extended_states = extended_states)","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"To obtain W_out, we can define a linear_model struct using one of the implemented constructors:","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"Ridge()\nLasso()\nElastNet()\nRobustHuber()","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"Each of them takes as input the regularization coefficient(s) and a MLJLinearModels.Solver as solver. Let's see a couple of examples. Using Ridge():","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"using MLJLinearModels\nlinear_model = Ridge(beta, Analytical())\nW_out = ESNtrain(linear_model, esn)\noutput = ESNpredict(esn, predict_len, W_out)","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"plot(transpose(output),layout=(3,1), label=\"predicted\")\nplot!(transpose(test),layout=(3,1), label=\"actual\")","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"(Image: esnRidgefixedseed)","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"Using Lasso():","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"beta = 0.0001\nlinear_model = Lasso(beta, ProxGrad(max_iter = 10000))\nW_out = ESNtrain(linear_model, esn)\noutput = ESNpredict(esn, predict_len, W_out)","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"plot(transpose(output),layout=(3,1), label=\"predicted\")\nplot!(transpose(test),layout=(3,1), label=\"actual\")","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"(Image: esnLassofixedseed)","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"Since in the linear model struct we used a MLJLinearModels.Solver, we can specify any parameter necessary, like in this case the max_iter.","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"Using ElastNet():","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"lambda = 0.1\ngamma = 0.0001\nlinear_model = ElastNet(lambda, gamma, ProxGrad(max_iter = 10000))\nW_out = ESNtrain(linear_model, esn)\noutput = ESNpredict(esn, predict_len, W_out)","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"plot(transpose(output),layout=(3,1), label=\"predicted\")\nplot!(transpose(test),layout=(3,1), label=\"actual\")","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"(Image: esnElastNetfixedseed)","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"Using RobustHuber():","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"delta = 0.5\nlambda = 0.1\ngamma = 0.0\nlinear_model = RobustHuber(lambda, gamma, LBFGS())\nW_out = ESNtrain(linear_model, esn)\noutput = ESNpredict(esn, predict_len, W_out)","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"plot(transpose(output),layout=(3,1), label=\"predicted\")\nplot!(transpose(test),layout=(3,1), label=\"actual\")","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"(Image: esnHuberfixedseed)","category":"page"},{"location":"examples/linear/#","page":"Using different linear methods","title":"Using different linear methods","text":"For the complete list of solvers please refer to MLJLinearModels solvers","category":"page"},{"location":"user/layers/#Layers-1","page":"Layers","title":"Layers","text":"","category":"section"},{"location":"user/layers/#Input-Layers-1","page":"Layers","title":"Input Layers","text":"","category":"section"},{"location":"user/layers/#","page":"Layers","title":"Layers","text":"    init_input_layer\n    init_dense_input_layer\n    init_sparse_input_layer\n    min_complex_input\n    irrational_sign_input","category":"page"},{"location":"user/layers/#ReservoirComputing.init_input_layer","page":"Layers","title":"ReservoirComputing.init_input_layer","text":"init_input_layer(res_size::Int, in_size::Int, sigma::Float64)\n\nReturn a weighted input layer matrix, with random non-zero elements drawn from $ [-\\text{sigma}, \\text{sigma}] $, as described in [1].\n\n[1] Lu, Zhixin, et al. \"Reservoir observers: Model-free inference of unmeasured variables in chaotic systems.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 27.4 (2017): 041102.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#ReservoirComputing.init_dense_input_layer","page":"Layers","title":"ReservoirComputing.init_dense_input_layer","text":"init_dense_input_layer(res_size::Int, in_size::Int, sigma::Float64)\n\nReturn a fully connected input layer matrix, with random non-zero elements drawn from $ [-sigma, sigma] $.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#ReservoirComputing.init_sparse_input_layer","page":"Layers","title":"ReservoirComputing.init_sparse_input_layer","text":"init_sparse_input_layer(res_size::Int, in_size::Int, sigma::Float64, sparsity::Float64)\n\nReturn a sparsely connected input layer matrix, with random non-zero elements drawn from $ [-sigma, sigma] $ and given sparsity.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#ReservoirComputing.min_complex_input","page":"Layers","title":"ReservoirComputing.min_complex_input","text":"min_complex_input(res_size::Int, in_size::Int, weight::Float64)\n\nReturn a fully connected input layer matrix with the same weights and sign drawn from a Bernoulli distribution, as described in [1].\n\n[1] Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#ReservoirComputing.irrational_sign_input","page":"Layers","title":"ReservoirComputing.irrational_sign_input","text":"irrational_sign_input(res_size::Int, in_size::Int , weight::Float64 [, start::Int, irrational::Irrational])\n\nReturn a fully connected input layer matrix with the same weights and sign decided by the values of an irrational number, as described in [1] and [2].\n\n[1] Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144. [2] Rodan, Ali, and Peter Tiňo. \"Simple deterministically constructed cycle reservoirs with regular jumps.\" Neural computation 24.7 (2012): 1822-1852.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#Reservoirs-1","page":"Layers","title":"Reservoirs","text":"","category":"section"},{"location":"user/layers/#","page":"Layers","title":"Layers","text":"    init_reservoir_givendeg\n    init_reservoir_givensp\n    pseudoSVD\n    DLR\n    DLRB\n    SCR\n    CRJ","category":"page"},{"location":"user/layers/#ReservoirComputing.init_reservoir_givendeg","page":"Layers","title":"ReservoirComputing.init_reservoir_givendeg","text":" init_reservoir_givendeg(res_size::Int, radius::Float64, degree::Int)\n\nReturn a reservoir matrix scaled by the radius value and with a given degree of connection.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#ReservoirComputing.init_reservoir_givensp","page":"Layers","title":"ReservoirComputing.init_reservoir_givensp","text":" init_reservoir_givensp(res_size::Int, radius::Float64, sparsity::Float64)\n\nReturn a reservoir matrix scaled by the radius value and with a given sparsity.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#ReservoirComputing.pseudoSVD","page":"Layers","title":"ReservoirComputing.pseudoSVD","text":"pseudoSVD(dim::Int,  max_value::Float64, sparsity::Float64 [, sorted::Bool, reverse_sort::Bool])\n\nReturn a reservoir matrix created using SVD as described in [1].\n\n[1] Yang, Cuili, et al. \"Design of polynomial echo state networks for time series prediction.\" Neurocomputing 290 (2018): 148-160.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#ReservoirComputing.DLR","page":"Layers","title":"ReservoirComputing.DLR","text":"DLR(res_size::Int, weight::Float64)\n\nReturn a Delay Line Reservoir matrix as described in [2].\n\n[2] Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#ReservoirComputing.DLRB","page":"Layers","title":"ReservoirComputing.DLRB","text":"DLRB(res_size::Int, weight::Float64, fb_weight::Float64)\n\nReturn a Delay Line Reservoir matrix with Backward connections as described in [2].\n\n[2] Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#ReservoirComputing.SCR","page":"Layers","title":"ReservoirComputing.SCR","text":"SCR(res_size::Int, weight::Float64)\n\nReturn a Simple Cycle Reservoir Reservoir matrix as described in [2].\n\n[2] Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#ReservoirComputing.CRJ","page":"Layers","title":"ReservoirComputing.CRJ","text":"CRJ(res_size::Int, cycle_weight::Float64, jump_weight::Float64, jump_size::Int)\n\nReturn a Cycle Reservoir with Jumps matrix as described in [2].\n\n[2] Rodan, Ali, and Peter Tiňo. \"Simple deterministically constructed cycle reservoirs with regular jumps.\" Neural computation 24.7 (2012): 1822-1852.\n\n\n\n\n\n","category":"function"},{"location":"user/layers/#","page":"Layers","title":"Layers","text":"Reservoirs only usable for RECA models:","category":"page"},{"location":"user/layers/#","page":"Layers","title":"Layers","text":"    ECA\n    GameOfLife","category":"page"},{"location":"user/layers/#ReservoirComputing.ECA","page":"Layers","title":"ReservoirComputing.ECA","text":"ECA(rule::Int, starting_val::AbstractArray{Int} [, generations::Int])\n\nCreates an Elementary Cellular Automaton [1] struct given the rule and a starting vector.\n\n[1] Wolfram, Stephen. A new kind of science. Vol. 5. Champaign, IL: Wolfram media, 2002.\n\n\n\n\n\n","category":"type"},{"location":"user/layers/#ReservoirComputing.GameOfLife","page":"Layers","title":"ReservoirComputing.GameOfLife","text":"function GameOfLife(initial_state::AbstractArray{T}, generations::Int)\n\nGiven a starting matrix return the evolution for given generations of Conway's Game of Life, as described in [1].\n\n[1] Gardner, Martin. “Mathematical games: The fantastic combinations of John Conway’s new solitaire game “life”.” Scientific American 223.4 (1970): 120-123.\n\n\n\n\n\n","category":"type"},{"location":"examples/layers/#Using-different-layers-1","page":"Using different layers","title":"Using different layers","text":"","category":"section"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"The composability of the ESN struct allows for the construction of the model with different reservoirs or input layers. In ReservoirComputing.jl there are different implementations of these layers found in the literature, but, of course, one is free to build a custom implementation. Following the prior example, we will continue to use the Lorenz system prediction as our test.","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"using ParameterizedFunctions, OrdinaryDiffEq\n\n#lorenz system parameters\nu0 = [1.0,0.0,0.0]                       \ntspan = (0.0,1000.0)                      \np = [10.0,28.0,8/3]\n\n#define lorenz system\nfunction lorenz(du,u,p,t)\n    du[1] = p[1]*(u[2]-u[1])\n    du[2] = u[1]*(p[2]-u[3]) - u[2]\n    du[3] = u[1]*u[2] - p[3]*u[3]\nend\n\n#solve system\nprob = ODEProblem(lorenz, u0, tspan, p)  \nsol = solve(prob, ABM54(), dt=0.02)   \nv = sol.u\ndata = Matrix(hcat(v...))\n\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#get data\ntrain = data[:, shift:shift+train_len-1]\ntest = data[:, shift+train_len:shift+train_len+predict_len-1]","category":"page"},{"location":"examples/layers/#Delay-Line-Reservoir-and-dense-input-layer-1","page":"Using different layers","title":"Delay Line Reservoir and dense input layer","text":"","category":"section"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"In order to change the default reservoir and input layer, we first need to define the ones we want to use. With the same parameters as the example before, we can define the ESN as follows:","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"using ReservoirComputing\n#model parameters\napprox_res_size = 300\nradius = 1.2\nactivation = tanh\ndegree = 6\nsigma = 0.1\nbeta = 0.0\nalpha = 1.0\nnla_type = NLADefault()\nextended_states = true\n\n#define the weight for the reservoir\nr= 0.95\n\n#define reservoir and input layer\nRandom.seed!(42) #fixed seed for reproducibility\nW = DLR(approx_res_size, r)\nW_in = init_dense_input_layer(approx_res_size, size(train, 1), sigma)\n\n#create echo state network  \nesndlr = ESN(W,\n    train,\n    W_in,\n    activation = activation,\n    alpha = alpha,\n    nla_type = nla_type,\n    extended_states = extended_states)\n\n#training and prediction\nW_out = ESNtrain(esndlr, beta)\noutput = ESNpredict(esndlr, predict_len, W_out)","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"And we can plot the results as before:","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"plot(transpose(output),layout=(3,1), label=\"predicted\")\nplot!(transpose(test),layout=(3,1), label=\"actual\")","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"(Image: esndlrfixedseed)","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"The reservoir used is taken from the paper [1]. The other reservoir illustrated therein are implemented in this package.","category":"page"},{"location":"examples/layers/#Pseudo-SVD-reservoir-and-irrational-input-layer-1","page":"Using different layers","title":"Pseudo SVD reservoir and irrational input layer","text":"","category":"section"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"Using another architecture just to have more examples, we can define the ESN using the reservoir obtained from the SVD-like method [2] and the irrational input layer, as described in [1]. We are going to use the same parameters as before, only adding the necessary ones for the construction of the new layers.","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"max_value = 1.5\nsparsity = 0.15\n\n#constructing the pseudo svd reservoir and irrational input layer\nW = pseudoSVD(approx_res_size, max_value, sparsity)\nW_in = irrational_sign_input(approx_res_size, size(train, 1), sigma)\n\nesnsvd = ESN(W,\n    train,\n    W_in,\n    activation = activation,\n    alpha = alpha,\n    nla_type = nla_type,\n    extended_states = extended_states)\n\n#training and prediction\nW_out = ESNtrain(esnsvd, beta)\noutput = ESNpredict(esnsvd, predict_len, W_out)","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"And, of course, the result can be plotted as always:","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"plot(transpose(output),layout=(3,1), label=\"predicted\")\nplot!(transpose(test),layout=(3,1), label=\"actual\")","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"(Image: esnsvdfixedseed)","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"As we can see, the results can vary wildly from one architecture to another, so be careful with the choice of the layers.","category":"page"},{"location":"examples/layers/#References-1","page":"Using different layers","title":"References","text":"","category":"section"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"[1]: Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.","category":"page"},{"location":"examples/layers/#","page":"Using different layers","title":"Using different layers","text":"[2]\" Yang, Cuili, et al. \"Design of polynomial echo state networks for time series prediction.\" Neurocomputing 290 (2018): 148-160.","category":"page"},{"location":"#Overview-1","page":"ReservoirComputing.jl","title":"Overview","text":"","category":"section"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Reservoir Computing is an umbrella term used to describe a family of models such as Echo State Networks (ESNs) and Liquid State Machines (LSMs). The key concept is to expand the input data into a higher dimension and use regression in order to train the model; in some ways Reservoir Computers can be considered similar to kernel methods.","category":"page"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"info: Introductory material\nThis library assumes some basic knowledge of Reservoir Computing. For a good introduction, we suggest the following papers: the first two are the seminal papers about ESN and LSM, the others are in-depth review papers that should cover all the needed information.Jaeger, Herbert: The “echo state” approach to analysing and training recurrent neural networks-with an erratum note.\nMaass W, Natschläger T, Markram H: Real-time computing without stable states: a new framework for neural computation based on perturbations.\nLukoševičius, Mantas: A practical guide to applying echo state networks.\" Neural networks: Tricks of the trade.\nLukoševičius, Mantas, and Herbert Jaeger: Reservoir computing approaches to recurrent neural network training.","category":"page"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"In this package (for the moment) there are the following models:","category":"page"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Echo State Networks (ESNs)\nSupport Vector Echo State Machines [1] (SVESMs)\nEcho State Gaussian Processes [2] (ESGPs)\nReservoir Computing with Cellular Automata [3] (RECAs)\nReservoir Memory Machine [4] (RMMs)\nDouble Activation Echo State Networks [5] (DAFESNs)","category":"page"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Multiple features are present as well, like the possibility of using a number of different reservoir and input layer architectures, as well as different linear regression methods. For more information on this please refer to the examples.","category":"page"},{"location":"#Installation-1","page":"ReservoirComputing.jl","title":"Installation","text":"","category":"section"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Since ReservoirComputing is registered in the Julia General Registry, it will suffice to do the following in the Julia REPL:","category":"page"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"]add ReservoirComputing","category":"page"},{"location":"#References-1","page":"ReservoirComputing.jl","title":"References","text":"","category":"section"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"[1]: Shi, Zhiwei, and Min Han. \"Support vector echo-state machine for chaotic time-series prediction.\" IEEE Transactions on Neural Networks 18.2 (2007): 359-372.","category":"page"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"[2]: Chatzis, Sotirios P., and Yiannis Demiris. \"Echo state Gaussian process.\" IEEE Transactions on Neural Networks 22.9 (2011): 1435-1445.","category":"page"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"[3]: Yilmaz, Ozgur. \"Reservoir computing using cellular automata.\" arXiv preprint arXiv:1410.0162 (2014).","category":"page"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"[4]: Paaßen, Benjamin, and Alexander Schulz. \"Reservoir memory machines.\" arXiv preprint arXiv:2003.04793 (2020).","category":"page"},{"location":"#","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"[5]: Lun, Shu-Xian, et al. \"A novel model of leaky integrator echo state network for time-series prediction.\" Neurocomputing 159 (2015): 58-66.","category":"page"},{"location":"user/linear/#Linear-Models-1","page":"Linear Models","title":"Linear Models","text":"","category":"section"},{"location":"user/linear/#Ridge-1","page":"Linear Models","title":"Ridge","text":"","category":"section"},{"location":"user/linear/#","page":"Linear Models","title":"Linear Models","text":"    Ridge","category":"page"},{"location":"user/linear/#ReservoirComputing.Ridge","page":"Linear Models","title":"ReservoirComputing.Ridge","text":"Ridge(lambda, solver::MLJLinearModels.Solver)\n\nReturn a LinearModel object for the training of the model using Ridge Regression with a MLJLinearModels method.\n\n\n\n\n\n","category":"type"},{"location":"user/linear/#Lasso-1","page":"Linear Models","title":"Lasso","text":"","category":"section"},{"location":"user/linear/#","page":"Linear Models","title":"Linear Models","text":"    Lasso","category":"page"},{"location":"user/linear/#ReservoirComputing.Lasso","page":"Linear Models","title":"ReservoirComputing.Lasso","text":"Lasso(lambda, solver::MLJLinearModels.Solver)\n\nReturn a LinearModel object for the training of the model using Lasso with a MLJLinearModels method.\n\n\n\n\n\n","category":"type"},{"location":"user/linear/#ElastNet-1","page":"Linear Models","title":"ElastNet","text":"","category":"section"},{"location":"user/linear/#","page":"Linear Models","title":"Linear Models","text":"    ElastNet","category":"page"},{"location":"user/linear/#ReservoirComputing.ElastNet","page":"Linear Models","title":"ReservoirComputing.ElastNet","text":"ElastNet(lambda, gamma, solver::MLJLinearModels.Solver)\n\nReturn a LinearModel object for the training of the model using Elastic Net with a MLJLinearModels method.\n\n\n\n\n\n","category":"type"},{"location":"user/linear/#Huber-1","page":"Linear Models","title":"Huber","text":"","category":"section"},{"location":"user/linear/#","page":"Linear Models","title":"Linear Models","text":"    RobustHuber","category":"page"},{"location":"user/linear/#ReservoirComputing.RobustHuber","page":"Linear Models","title":"ReservoirComputing.RobustHuber","text":"ElastNet(delta, lambda, gamma, solver::MLJLinearModels.Solver)\n\nReturn a LinearModel object for the training of the model using the Huber function with a MLJLinearModels method.\n\n\n\n\n\n","category":"type"},{"location":"user/spec/#Special-ESNs-1","page":"Special ESNs","title":"Special ESNs","text":"","category":"section"},{"location":"user/spec/#","page":"Special ESNs","title":"Special ESNs","text":"These ESN are \"special\" in the fact that they have their own training methods. One of the future goals is to merge all the training methods into one. For all these models the constructor is the same, ESN.","category":"page"},{"location":"user/spec/#Support-Vector-Echo-State-Machines-1","page":"Special ESNs","title":"Support Vector Echo State Machines","text":"","category":"section"},{"location":"user/spec/#","page":"Special ESNs","title":"Special ESNs","text":"Train ","category":"page"},{"location":"user/spec/#","page":"Special ESNs","title":"Special ESNs","text":"    SVESMtrain","category":"page"},{"location":"user/spec/#ReservoirComputing.SVESMtrain","page":"Special ESNs","title":"ReservoirComputing.SVESMtrain","text":"SVESMtrain(svr::LIBSVM.AbstractSVR, esn::AbstractLeakyESN [, y_target::AbstractArray{Float64}])\n\nTrain the ESN using SVM regression, as described in [1].\n\n[1] Shi, Zhiwei, and Min Han. \"Support vector echo-state machine for chaotic time-series prediction.\" IEEE Transactions on Neural Networks 18.2 (2007): 359-372.\n\n\n\n\n\n","category":"function"},{"location":"user/spec/#","page":"Special ESNs","title":"Special ESNs","text":"Predict","category":"page"},{"location":"user/spec/#","page":"Special ESNs","title":"Special ESNs","text":"    SVESM_direct_predict\n    SVESMpredict\n    SVESMpredict_h_steps","category":"page"},{"location":"user/spec/#ReservoirComputing.SVESM_direct_predict","page":"Special ESNs","title":"ReservoirComputing.SVESM_direct_predict","text":"SVESM_direct_predict(esn::AbstractLeakyESN, test_in::AbstractArray{Float64}, fitted_svr::LIBSVM.AbstractSVR)\n\nGiven the input data return the corresponding predicted output, as described in [1].\n\n[1] Shi, Zhiwei, and Min Han. \"Support vector echo-state machine for chaotic time-series prediction.\" IEEE Transactions on Neural Networks 18.2 (2007): 359-372.\n\n\n\n\n\n","category":"function"},{"location":"user/spec/#ReservoirComputing.SVESMpredict","page":"Special ESNs","title":"ReservoirComputing.SVESMpredict","text":"SVESMpredict(esn::AbstractLeakyESN, predict_len::Int, fitted_svr::AbstractArray{Any})\n\nReturn the prediction for a given length of the constructed ESN struct using SVMs.\n\n\n\n\n\n","category":"function"},{"location":"user/spec/#ReservoirComputing.SVESMpredict_h_steps","page":"Special ESNs","title":"ReservoirComputing.SVESMpredict_h_steps","text":"SVESMpredict_h_steps(esn::AbstractLeakyESN, predict_len::Int, h_steps::Int, test_data::AbstractArray{Float64}, fitted_svr::LIBSVM.AbstractSVR)\n\nReturn the prediction for h steps ahead of the constructed ESN struct using SVMs.\n\n\n\n\n\n","category":"function"},{"location":"user/spec/#Echo-State-Gaussian-Processes-1","page":"Special ESNs","title":"Echo State Gaussian Processes","text":"","category":"section"},{"location":"user/spec/#","page":"Special ESNs","title":"Special ESNs","text":"Train","category":"page"},{"location":"user/spec/#","page":"Special ESNs","title":"Special ESNs","text":"    ESGPtrain","category":"page"},{"location":"user/spec/#ReservoirComputing.ESGPtrain","page":"Special ESNs","title":"ReservoirComputing.ESGPtrain","text":"ESGPtrain(esn::AbstractLeakyESN, mean::GaussianProcesses.Mean, kernel::GaussianProcesses.Kernel\n[, lognoise::Float64, optimize::Bool, optimizer::Optim.AbstractOptimizer, y_target::AbstractArray{Float64})\n\nTrain the ESN using Gaussian Processes, as described in [1]\n\n[1] Chatzis, Sotirios P., and Yiannis Demiris. \"Echo state Gaussian process.\" IEEE Transactions on Neural Networks 22.9 (2011): 1435-1445.\n\n\n\n\n\n","category":"function"},{"location":"user/spec/#","page":"Special ESNs","title":"Special ESNs","text":"Predict","category":"page"},{"location":"user/spec/#","page":"Special ESNs","title":"Special ESNs","text":"    ESGPpredict\n    ESGPpredict_h_steps","category":"page"},{"location":"user/spec/#ReservoirComputing.ESGPpredict","page":"Special ESNs","title":"ReservoirComputing.ESGPpredict","text":"ESGPpredict(esn::AbstractLeakyESN, predict_len::Int, gp::AbstractArray{Any})\n\nReturn the prediction for a given length of the constructed ESN struct using GPs.\n\n\n\n\n\n","category":"function"},{"location":"user/spec/#ReservoirComputing.ESGPpredict_h_steps","page":"Special ESNs","title":"ReservoirComputing.ESGPpredict_h_steps","text":"ESGPpredict_h_steps(esn::AbstractLeakyESN, predict_len::Int, h_steps::Int, test_data::AbstractArray{Float64}, gp::GaussianProcesses.GPE)\n\nReturn the prediction for h steps ahead of the constructed ESN struct using GPs.\n\n\n\n\n\n","category":"function"}]
}
