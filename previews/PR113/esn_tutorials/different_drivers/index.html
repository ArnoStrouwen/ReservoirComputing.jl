<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Using Different Reservoir Drivers · ReservoirComputing.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><script src="../../../copy.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="ReservoirComputing.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ReservoirComputing.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">ReservoirComputing.jl</a></li><li><span class="tocitem">General Settings</span><ul><li><a class="tocitem" href="../../general/different_training/">Changing Training Algorithms</a></li><li><a class="tocitem" href="../../general/states_variation/">Altering States</a></li><li><a class="tocitem" href="../../general/predictive_generative/">Generative vs Predictive</a></li></ul></li><li><span class="tocitem">Echo State Network Tutorials</span><ul><li><a class="tocitem" href="../lorenz_basic/">Lorenz System Forecasting</a></li><li><a class="tocitem" href="../change_layers/">Using Different Layers</a></li><li class="is-active"><a class="tocitem" href>Using Different Reservoir Drivers</a><ul class="internal"><li><a class="tocitem" href="#Multiple-Activation-Function-RNN"><span>Multiple Activation Function RNN</span></a></li><li><a class="tocitem" href="#Gated-Recurrent-Unit"><span>Gated Recurrent Unit</span></a></li></ul></li><li><a class="tocitem" href="../different_training/">Using Different Training Methods</a></li><li><a class="tocitem" href="../hybrid/">Hybrid Echo State Networks</a></li></ul></li><li><span class="tocitem">Reservoir Computing with Cellular Automata</span><ul><li><a class="tocitem" href="../../reca_tutorials/reca/">5 Bit Memory Task</a></li></ul></li><li><span class="tocitem">API Documentation</span><ul><li><a class="tocitem" href="../../api/training/">Training Algorithms</a></li><li><a class="tocitem" href="../../api/states/">States Modifications</a></li><li><a class="tocitem" href="../../api/predict/">Prediction Types</a></li><li><a class="tocitem" href="../../api/esn/">Echo State Networks</a></li><li><a class="tocitem" href="../../api/esn_layers/">ESN Layers</a></li><li><a class="tocitem" href="../../api/esn_drivers/">ESN Drivers</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Echo State Network Tutorials</a></li><li class="is-active"><a href>Using Different Reservoir Drivers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Using Different Reservoir Drivers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/ReservoirComputing.jl/blob/master/docs/src/esn_tutorials/different_drivers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Using-Different-Reservoir-Drivers"><a class="docs-heading-anchor" href="#Using-Different-Reservoir-Drivers">Using Different Reservoir Drivers</a><a id="Using-Different-Reservoir-Drivers-1"></a><a class="docs-heading-anchor-permalink" href="#Using-Different-Reservoir-Drivers" title="Permalink"></a></h1><p>While the original implementation of the Echo State Network implemented the model using the equations of Recurrent Neural Networks to obtain non linearity in the reservoir, other variations have been proposed in recent years. More specifically the different drivers implemented in ReservoirComputing.jl are the multiple activation function RNN <code>MRNN()</code> and the Gated Recurrent Unit <code>GRU()</code>. To change them it suffice to give the chosen method to the <code>ESN</code> keyword argument <code>reservoir_driver</code>. In this section some example of their usage will be given, as well as a quick introduction to their euqations.</p><h2 id="Multiple-Activation-Function-RNN"><a class="docs-heading-anchor" href="#Multiple-Activation-Function-RNN">Multiple Activation Function RNN</a><a id="Multiple-Activation-Function-RNN-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Activation-Function-RNN" title="Permalink"></a></h2><p>Based on the double activation function ESN (DAFESN) proposed in <sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>, the Multiple Activation Function ESN expands the idea and allows a custom number of activation functions to be used in the reservoir dynamics. This can be thought as a linear combination of multiple activation functions with corresponding parameters.</p><p class="math-container">\[\mathbf{x}(t+1) = (1-\alpha)\mathbf{x}(t) + \lambda_1 f_1(\mathbf{W}\mathbf{x}(t)+\mathbf{W}_{in}\mathbf{u}(t)) + \dots + \lambda_D f_D(\mathbf{W}\mathbf{x}(t)+\mathbf{W}_{in}\mathbf{u}(t))\]</p><p>where <span>$D$</span> is the number of activation function and respective parameters chosen.</p><p>The method to call to use the mutliple activation function ESN is <code>MRNN(activation_function, leaky_coefficient, scaling_factor)</code>. The arguments can be used as both <code>args</code> or <code>kwargs</code>. <code>activation_function</code> and <code>scaling_factor</code> have to be vectors (or tuples) containing the chosen activation functions and respective scaling factors (<span>$f_1,...,f_D$</span> and <span>$\lambda_1,...,\lambda_D$</span> following the nomenclature introduced above). The leaky_coefficient represents <span>$\alpha$</span> and it is a single value. </p><p>Starting the example, the data used is based on the following function based on the DAFESN paper <sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>. A full script of the example is available <a href="../scripts/change_drivers_mrnn.jl">here</a>.</p><pre><code class="language-julia hljs">u(t) = sin(t)+sin(0.51*t)+sin(0.22*t)+sin(0.1002*t)+sin(0.05343*t)</code></pre><p>For this example the type of prediction will be one step ahead. The metric used to assure a good prediction is going to be the normalized root-mean-square deviation <code>rmsd</code> from <a href="https://juliastats.org/StatsBase.jl/stable/">StatsBase</a>. Like in the other examples first it is needed to gather the data:</p><pre><code class="language-julia hljs">data = u.(collect(0.0:0.01:500))
training_input = reduce(hcat, data[shift:shift+train_len-1])
training_target = reduce(hcat, data[shift+1:shift+train_len])
testing_input = reduce(hcat, data[shift+train_len:shift+train_len+predict_len-1])
testing_target = reduce(hcat, data[shift+train_len+1:shift+train_len+predict_len])</code></pre><p>In order to follow the paper more closely it is necessary to define a couple of activation functions. The numbering of them follows the ones in the paper. Of course one can also use any function, custom defined, available in the base language or any activation function from <a href="https://fluxml.ai/Flux.jl/stable/models/nnlib/#Activation-Functions">NNlib</a>.</p><pre><code class="language-julia hljs">f2(x) = (1-exp(-x))/(2*(1+exp(-x)))
f3(x) = (2/pi)*atan((pi/2)*x)
f4(x) = x/sqrt(1+x*x)</code></pre><p>It is now possible to build different drivers, using the paramters suggested by the paper. Also in this instance the numbering follows the test cases of the paper. In the end a simple for loop is implemented to compare the different drivers and activation functions.</p><pre><code class="language-julia hljs">using Reservoir Computing, Random

#fix seed for reproducibility
Random.seed!(42)

#baseline case with RNN() driver. Parameter given as args
base_case = RNN(tanh, 0.85)

#MRNN() test cases
#Parameter given as kwargs
case3 = MRNN(activation_function=[tanh, f2], 
    leaky_coefficient=0.85, 
    scaling_factor=[0.5, 0.3])

#Parameter given as kwargs
case4 = MRNN(activation_function=[tanh, f3], 
    leaky_coefficient=0.9, 
    scaling_factor=[0.45, 0.35])

#Parameter given as args
case5 = MRNN([tanh, f4], 0.9, [0.43, 0.13])

#tests
test_cases = [base_case, case3, case4, case5]
for case in test_cases
    esn = ESN(100, training_input,
        input_init = WeightedLayer(scaling=0.3),
        reservoir_init = RandSparseReservoir(radius=0.4),
        reservoir_driver = case,
        states_type = ExtendedStates())
    wout = train(esn, training_target, StandardRidge(10e-6))
    output = esn(Predictive(testing_input), wout)
    println(rmsd(testing_target, output, normalize=true))
end</code></pre><pre><code class="nohighlight hljs">1.2859434466604239e-5
2.1753694726497823e-5
2.9563481223700186e-5
2.5164499914117052e-5</code></pre><p>In this example it is also possible to observe the input of parameters to the methods <code>RNN()</code> <code>MRNN()</code> both by argument and by keyword argument.</p><h2 id="Gated-Recurrent-Unit"><a class="docs-heading-anchor" href="#Gated-Recurrent-Unit">Gated Recurrent Unit</a><a id="Gated-Recurrent-Unit-1"></a><a class="docs-heading-anchor-permalink" href="#Gated-Recurrent-Unit" title="Permalink"></a></h2><p>Gated Recurrent Units (GRUs) <sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> have been proposed in more recent years with the intent of limiting notable problems of RNNs, like the vanishing gradient. This change in the underlying equations can be easily transported in the Reservoir Computing paradigm, switching the RNN equations in the reservoir with the GRU equations. This approach has been explored in <sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup> and <sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup>. Different variations of GRU have been proposed <sup class="footnote-reference"><a id="citeref-5" href="#footnote-5">[5]</a></sup><sup class="footnote-reference"><a id="citeref-6" href="#footnote-6">[6]</a></sup>; this section is subdivided into different sections that go in detail about the governing equations and the implementation of them into ReservoirComputing.jl. Like before, to access the GRU reservoir driver it suffice to change the <code>reservoir_diver</code> keyword argument for <code>ESN</code> with <code>GRU()</code>. All the variations that are going to be presented can be used in this package by leveraging the keyword argument <code>variant</code> in the method <code>GRU()</code> and specifying the chosen variant: <code>Variant1()</code>, <code>Variant2()</code>, <code>Variant3()</code> or <code>Minimal()</code>. The default is set to the standard version <code>FullyGated()</code>. The first section will go in more detail about the default of the <code>GRU()</code> method, and the following ones will refer to it to minimize repetitions.</p><h3 id="Standard-GRU"><a class="docs-heading-anchor" href="#Standard-GRU">Standard GRU</a><a id="Standard-GRU-1"></a><a class="docs-heading-anchor-permalink" href="#Standard-GRU" title="Permalink"></a></h3><p>The equations for the standard GRU are as follows:</p><p class="math-container">\[\mathbf{r}(t) = \sigma (\mathbf{W}^r_{\text{in}}\mathbf{u}(t)+\mathbf{W}^r\mathbf{x}(t-1)+\mathbf{b}_r) \\
\mathbf{z}(t) = \sigma (\mathbf{W}^z_{\text{in}}\mathbf{u}(t)+\mathbf{W}^z\mathbf{x}(t-1)+\mathbf{b}_z) \\
\tilde{\mathbf{x}}(t) = \text{tanh}(\mathbf{W}_{in}\mathbf{u}(t)+\mathbf{W}(\mathbf{r}(t) \odot \mathbf{x}(t-1))+\mathbf{b}) \\
\mathbf{x}(t) = \mathbf{z}(t) \odot \mathbf{x}(t-1)+(1-\mathbf{z}(t)) \odot \tilde{\mathbf{x}}(t)\]</p><p>Going over the <code>GRU</code> keyword argument it will be explained how to feed the desired input to the model. </p><ul><li><code>activation_function</code> is a vector with default values <code>[NNlib.sigmoid, NNlib.sigmoid, tanh]</code>. This argument controls the activation functions of the GRU, going from top to bottom. Changing the first element corresponds in changing the activation function for <span>$\mathbf{r}(t)$</span>.</li><li><code>layer_init</code> is a vector with default values <code>fill(DenseLayer(), 5)</code>. This keyword argument controls the <span>$\mathbf{W}_{\text{in}}$</span>s and the <span>$\mathbf{b}$</span>s going from top to bottom, left to right. For example, changing the first element will change <span>$\mathbf{W}^r_{\text{in}}$</span>, changing the second will change <span>$\mathbf{b}_r$</span> and so on.</li><li><code>reservoir_init</code> is a vector with default value <code>fill(RandSparseReservoir(), 2)</code>. In a similar fashion to <code>layer_init</code>, this keyword argument controls the reservoir matrix construction in a top to bottom order. </li><li><code>variant</code> as already illustrated controls the GRU variant. The default value is set to <code>FullyGated()</code>.</li></ul><p>It is important to notice that <code>layer_init</code> and <code>reservoir_init</code> control every layer except <span>$\mathbf{W}_{in}$</span> and <span>$\mathbf{W}$</span>. These arguments are given as input to the <code>ESN()</code> call as usual.</p><h3 id="Type-1"><a class="docs-heading-anchor" href="#Type-1">Type 1</a><a id="Type-1-1"></a><a class="docs-heading-anchor-permalink" href="#Type-1" title="Permalink"></a></h3><p>The first variation of the GRU is dependent only on the previous hidden state and the bias:</p><p class="math-container">\[\mathbf{r}(t) = \sigma (\mathbf{W}^r\mathbf{x}(t-1)+\mathbf{b}_r) \\
\mathbf{z}(t) = \sigma (\mathbf{W}^z\mathbf{x}(t-1)+\mathbf{b}_z) \\\]</p><p>This means that <code>layer_init</code> is 3-dimensional instead of 5 given the absence of <span>$\mathbf{W}_{in}$</span>s.</p><h3 id="Type-2"><a class="docs-heading-anchor" href="#Type-2">Type 2</a><a id="Type-2-1"></a><a class="docs-heading-anchor-permalink" href="#Type-2" title="Permalink"></a></h3><p>The second variation only depends on the previous hiddens state:</p><p class="math-container">\[\mathbf{r}(t) = \sigma (\mathbf{W}^r\mathbf{x}(t-1)) \\
\mathbf{z}(t) = \sigma (\mathbf{W}^z\mathbf{x}(t-1)) \\\]</p><p>Here <code>layer_init</code> only has one element, and it control the bias vector of <span>$\tilde{\mathbf{x}}(t)$</span>.</p><h3 id="Type-3"><a class="docs-heading-anchor" href="#Type-3">Type 3</a><a id="Type-3-1"></a><a class="docs-heading-anchor-permalink" href="#Type-3" title="Permalink"></a></h3><p>The final variation before the minimal one depends only on the biases</p><p class="math-container">\[\mathbf{r}(t) = \sigma (\mathbf{b}_r) \\
\mathbf{z}(t) = \sigma (\mathbf{b}_z) \\\]</p><p>This means that <code>layer_init</code> is 3-dimensional and it controls only the bias vectors. </p><h3 id="Minimal"><a class="docs-heading-anchor" href="#Minimal">Minimal</a><a id="Minimal-1"></a><a class="docs-heading-anchor-permalink" href="#Minimal" title="Permalink"></a></h3><p>The minimal GRU variation merges two gates into one:</p><p class="math-container">\[\mathbf{f}(t) = \sigma (\mathbf{W}^f_{\text{in}}\mathbf{u}(t)+\mathbf{W}^f\mathbf{x}(t-1)+\mathbf{b}_f) \\
\tilde{\mathbf{x}}(t) = \text{tanh}(\mathbf{W}_{in}\mathbf{u}(t)+\mathbf{W}(\mathbf{f}(t) \odot \mathbf{x}(t-1))+\mathbf{b}) \\
\mathbf{x}(t) = (1-\mathbf{f}(t)) \odot \mathbf{x}(t-1) + \mathbf{f}(t) \odot \tilde{\mathbf{x}}(t)\]</p><p>As a consequence <code>layer_init</code> is 3-dimensional and <code>reservoir_init</code> is 1-dimensional</p><h3 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h3><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Lun, Shu-Xian, et al. &quot;<em>A novel model of leaky integrator echo state network for time-series prediction.</em>&quot; Neurocomputing 159 (2015): 58-66.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>Cho, Kyunghyun, et al. “<em>Learning phrase representations using RNN encoder-decoder for statistical machine translation.</em>” arXiv preprint arXiv:1406.1078 (2014).</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>Wang, Xinjie, Yaochu Jin, and Kuangrong Hao. &quot;<em>A Gated Recurrent Unit based Echo State Network.</em>&quot; 2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 2020.</li><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a>Di Sarli, Daniele, Claudio Gallicchio, and Alessio Micheli. &quot;<em>Gated Echo State Networks: a preliminary study.</em>&quot; 2020 International Conference on INnovations in Intelligent SysTems and Applications (INISTA). IEEE, 2020.</li><li class="footnote" id="footnote-5"><a class="tag is-link" href="#citeref-5">5</a>Dey, Rahul, and Fathi M. Salem. &quot;<em>Gate-variants of gated recurrent unit (GRU) neural networks.</em>&quot; 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS). IEEE, 2017.</li><li class="footnote" id="footnote-6"><a class="tag is-link" href="#citeref-6">6</a>Zhou, Guo-Bing, et al. &quot;<em>Minimal gated unit for recurrent neural networks.</em>&quot; International Journal of Automation and Computing 13.3 (2016): 226-234.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../change_layers/">« Using Different Layers</a><a class="docs-footer-nextpage" href="../different_training/">Using Different Training Methods »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.12 on <span class="colophon-date" title="Friday 4 February 2022 17:10">Friday 4 February 2022</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
