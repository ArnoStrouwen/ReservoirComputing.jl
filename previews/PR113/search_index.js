var documenterSearchIndex = {"docs":
[{"location":"esn_tutorials/gruesn/#Gated-Echo-State-Networks","page":"Gated Echo State Networks","title":"Gated Echo State Networks","text":"","category":"section"},{"location":"esn_tutorials/lorenz_basic/#Lorenz-System-Forecasting","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"","category":"section"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"This example expands on the readme Lorenz system forecasting to better showcase how to use methods and functions provided in the library for Echo State Networks. Here the prediction method used is Generative, for a more detailed explanation of the differences between Generative and Predictive please refer to the following examples given in the literature. ","category":"page"},{"location":"esn_tutorials/lorenz_basic/#Harvesting-the-data","page":"Lorenz System Forecasting","title":"Harvesting the data","text":"","category":"section"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"Starting off the workflow the first step is to obtain the data. Leveraging OrdinaryDiffEq it is possible to derive the Lorenz system data in the following way:","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"using OrdinaryDiffEq\n\n#define lorenz system\nfunction lorenz!(du,u,p,t)\n    du[1] = 10.0*(u[2]-u[1])\n    du[2] = u[1]*(28.0-u[3]) - u[2]\n    du[3] = u[1]*u[2] - (8/3)*u[3]\nend\n\n#solve and take data\nprob = ODEProblem(lorenz!, [1.0,0.0,0.0], (0.0,200.0))\ndata = solve(prob, ABM54(), dt=0.02)","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"After obtaining the data it is necessary to determine the kind of prediction for the model. Since this example is going to use the Generative prediction type, this means that the target data is foing to be the next step of the input data. In addition it is important to notice that the Lorenz system just obtained presents a transient period that is not representative of the general behavior of the system. This can easily be discarded setting a shift parameter.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"#determine shift length, training length and prediction length\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#split the data accordingly\ninput_data = data[:, shift:shift+train_len-1]\ntarget_data = data[:, shift+1:shift+train_len]\ntest_data = data[:,shift+train_len+1:shift+train_len+predict_len]","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"It is important to notice that the data needs to be formatted in a matrix with the features as rows and timesteps as columns like it is done in this example. This is needed even if the timeserie consists of single values. ","category":"page"},{"location":"esn_tutorials/lorenz_basic/#Building-the-Echo-State-Network","page":"Lorenz System Forecasting","title":"Building the Echo State Network","text":"","category":"section"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"Once the data is ready it is possible to define the parameters for the ESN and the ESN struct itself. In this example the values from [1] are loosely followed as general guidelines.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"using ReservoirComputing\n\n#define ESN parameters\nres_size = 300\nres_radius = 1.2\nres_sparsity = 6/300\ninput_scaling = 0.1\n\n#build ESN struct\nesn = ESN(res_size, input_data; \n    variation = Default(),\n    reservoir_init = RandSparseReservoir(radius=res_radius, sparsity=res_sparsity),\n    input_init = WeightedLayer(scaling=input_scaling),\n    reservoir_driver = RNN(),\n    nla_type = NLADefault(),\n    states_type = StandardStates())","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"Most of the parameters here chosen mirror the default ones, so a direct call is not necessary. The readme example is identical to this one, except for the explicit call. Going line by line to see what is happening starting from res_size: this value determines the dimensions of the reservoir matrix. In this case a size of 300 has been chose, so the reservoir matrix is going to be 300 x 300. This is not always the case, since some input layer constructions can modify the dimensions of the reservoir, but in that case everything is taken care of internally. ","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"The res_radius determines the scaling of the spectral radius of the reservoir matrix; a proper scaling is necessary to assure the Echo State Property. The default value in the RandSparseReservoir() method is 1.0 in accordance to the most followed guidelines found in the literature (see [2] and references therein). The sparsity of the reservoir matrix in this case is obtained by choosing a degree of connections and dividing that by the reservoir size. Of course it is also possible to simply choose any value between 0.0 and 1.0 to test behaviors for different sparsity values. In this example the call to the parameters inside RandSparseReservoir() was done explicitly to showcase the meaning of each of them, but it is aslo possible to simply pass the values directly like so RandSparseReservoir(1.2, 6/300).","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"The value of input_scaling determines the upper and lower bounds of the uniform distribution of the weights in the WeightedLayer(). Like before this value can be passed either as argument or keyword argument WeightedLayer(0.1). The value of 0.1 represents the default. The default input layer is the DenseLayer, a fully connected layer. The details of the weighted version can be found in [3], for this example this version returns the best results.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"The reservoir driver represents the dynamics of the reservoir. In the standard ESN definition these dynamics are obtained through a Recurrent Neural Network (RNN), and this is reflected by calling the RNN driver for the ESN struct. This option is set as the default and unless there is the need to change parameters it is not needed. The full equation is the following:","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"textbfx(t+1) = (1-alpha)textbfx(t) + alpha cdot texttanh(textbfWtextbfx(t)+textbfW_textintextbfu(t))","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"where Î± represents the leaky coefficient and tanh can be any activation function. Also textbfx represent the state vector, textbfu the input data and textbfW textbfW_textin are the reservoir matrix and input matrix respectively. The default call to the RNN in the library is the following RNN(;activation_function=tanh, leaky_coefficient=1.0), where the meaning of the parameters is clear from the equation above. Instead og the hyperbolic tangent any activation function can be used, either leveraging external lybraries such as NNlib or creating a custom one. ","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"The final calls are modifications to the states in training or prediction. The default calls, depicted in the example, do not apport any modifications to the states. This is the safest bet is one is not sure on how these work. The nla_type applies a non linear algorithm to the states, while the states_type can expand them concatenating them with the inpu data, or padding them concatenating a constant value to all the states. More in depth descriptions about these parameters are given in other examples in the documentation.","category":"page"},{"location":"esn_tutorials/lorenz_basic/#Training-and-Prediction","page":"Lorenz System Forecasting","title":"Training and Prediction","text":"","category":"section"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"Now that the ESN has been created and all the parameters have been explained it is time to proceed with the training. The full call of the readme example follows this general idea:","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"#define training method\ntraining_method = StandardRidge(0.0)\n\n#obtain output layer\noutput_layer = train(esn, target_data, training_method)","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"The training returns an OutputLayer struct containing the trained output matrix and other informations needed for the prediction. The necessary elements in the train() call are the ESN struct created in the previous step and the target_data, that in this case is the one step ahead evolution of the Lorenz system. The training method chosen in this example is the standard one, so an equivalent way of calling the train function here is output_layer = train(esn, target_data) like the readme basic version. Likewise the default value for the ridge regression parameter is set to zero, so the actual default training is Ordinary Least Squares regression. Other training methods are available and will be explained in following examples. ","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"Once the OutputLayer has been obtained the prediction can be done following this procedure:","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"output = esn(Generative(predict_len), output_layer)","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"both the training method and the output layer are needed in this call. The number of steps for the prediction must be specified to the Generative method. The output results are given in a matrix.  To inspect the results they can easily be plotted using an external library. In this case Plots is adopted:","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"using Plots\nplot(transpose(output),layout=(3,1), label=\"predicted\")\nplot!(transpose(test_data),layout=(3,1), label=\"actual\")","category":"page"},{"location":"esn_tutorials/lorenz_basic/#Bibliography","page":"Lorenz System Forecasting","title":"Bibliography","text":"","category":"section"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"[1]: Pathak, Jaideep, et al. \"Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 27.12 (2017): 121102.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"[2]: LukoÅ¡eviÄius, Mantas. \"A practical guide to applying echo state networks.\" Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 659-686.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"[3]: Lu, Zhixin, et al. \"Reservoir observers: Model-free inference of unmeasured variables in chaotic systems.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 27.4 (2017): 041102.","category":"page"},{"location":"esn_tutorials/different_training/#Changing-Training-Algorithms","page":"Changing Training Algorithms","title":"Changing Training Algorithms","text":"","category":"section"},{"location":"api/predict/#Prediction-Types","page":"Prediction Types","title":"Prediction Types","text":"","category":"section"},{"location":"api/predict/","page":"Prediction Types","title":"Prediction Types","text":"    Generative\n    Predictive","category":"page"},{"location":"api/predict/#ReservoirComputing.Generative","page":"Prediction Types","title":"ReservoirComputing.Generative","text":"Generative(prediction_len)\n\nThis prediction methodology allows the models to produce an autonomous prediction, feeding the prediction into itself to generate the next step.  The only parameter needed is the number of steps for the prediction.\n\n\n\n\n\n","category":"type"},{"location":"api/predict/#ReservoirComputing.Predictive","page":"Prediction Types","title":"ReservoirComputing.Predictive","text":"Predictive(prediction_data)\n\nGiven a set of labels as prediction_data this method of prediction will return the correspinding labels in a standard Machine Learning fashion.\n\n\n\n\n\n","category":"type"},{"location":"api/reca/#Reservoir-Computing-with-Cellular-Automata","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"","category":"section"},{"location":"api/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"    RECA","category":"page"},{"location":"api/reca/#ReservoirComputing.RECA","page":"Reservoir Computing with Cellular Automata","title":"ReservoirComputing.RECA","text":"RECA(train_data,\n    automata;\n    generations = 8,\n    input_encoding=RandomMapping(),\n    nla_type = NLADefault(),\n    states_type = StandardStates())\n\n[1] Yilmaz, Ozgur. âReservoir computing using cellular automata.â arXiv preprint arXiv:1410.0162 (2014).\n\n[2] Nichele, Stefano, and Andreas Molund. âDeep reservoir computing using cellular automata.â arXiv preprint arXiv:1703.02806 (2017).\n\n\n\n\n\n","category":"type"},{"location":"api/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"The input encodings are the equivalent of the input matrices of the ESNs. These are the available encodings:","category":"page"},{"location":"api/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"    RandomMapping","category":"page"},{"location":"api/reca/#ReservoirComputing.RandomMapping","page":"Reservoir Computing with Cellular Automata","title":"ReservoirComputing.RandomMapping","text":"RandomMapping(permutations, expansion_size)\nRandomMapping(permutations; expansion_size=40)\nRandomMapping(;permutations=8, expansion_size=40)\n\nRandom mapping of the input data directly in the reservoir. The expansion_size determines the dimension of the single reservoir, and  permutations determines the number of total reservoirs that will be connected, each with a different mapping. The detail of this  implementation can be found in [1].\n\n[1] Nichele, Stefano, and Andreas Molund. âDeep reservoir computing using cellular automata.â arXiv preprint arXiv:1703.02806 (2017).\n\n\n\n\n\n","category":"type"},{"location":"api/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"The training and prediction follow the same workflow of the ESN. It is important to note that at the moment we were not able to find any paper using these models with a Generative approach for the prediction, so full support is given only to the Predictive method.","category":"page"},{"location":"esn_tutorials/change_layers/#Using-Different-Layers","page":"Using Different Layers","title":"Using Different Layers","text":"","category":"section"},{"location":"esn_tutorials/predictive_generative/#Generative-vs-Predictive","page":"Generative vs Predictive","title":"Generative vs Predictive","text":"","category":"section"},{"location":"api/states/#States-Modifications","page":"States Modifications","title":"States Modifications","text":"","category":"section"},{"location":"api/states/#Padding-and-Estension","page":"States Modifications","title":"Padding and Estension","text":"","category":"section"},{"location":"api/states/","page":"States Modifications","title":"States Modifications","text":"    StandardStates\n    ExtendedStates\n    PaddedStates\n    PaddedExtendedStates","category":"page"},{"location":"api/states/#ReservoirComputing.StandardStates","page":"States Modifications","title":"ReservoirComputing.StandardStates","text":"StandardStates()\n\nNo modification of the states takes place, default option.\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.ExtendedStates","page":"States Modifications","title":"ReservoirComputing.ExtendedStates","text":"ExtendedStates()\n\nThe states are extended with the input data, for the training section, and the prediction data,  during the prediction section. This is obtained with a vertical concatenation of the data and the states.\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.PaddedStates","page":"States Modifications","title":"ReservoirComputing.PaddedStates","text":"PaddedStates(padding)\nPaddedStates(;padding=1.0)\n\nThe states are padded with a chosen value. Usually this value is set to one. The padding is obtained through a  vertical concatenation of the padding value and the states.\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.PaddedExtendedStates","page":"States Modifications","title":"ReservoirComputing.PaddedExtendedStates","text":"PaddedExtendedStates(padding)\nPaddedExtendedStates(;padding=1.0)\n\nThe states are extended with the training data or predicted data and subsequently padded with a chosen value.  Usually the padding value is set to one. The padding and the extension are obtained through a vertical concatenation  of the padding value, the data and the states.\n\n\n\n\n\n","category":"type"},{"location":"api/states/#Non-Linear-Transformations","page":"States Modifications","title":"Non Linear Transformations","text":"","category":"section"},{"location":"api/states/","page":"States Modifications","title":"States Modifications","text":"    NLADefault\n    NLAT1\n    NLAT2\n    NLAT3","category":"page"},{"location":"api/states/#ReservoirComputing.NLADefault","page":"States Modifications","title":"ReservoirComputing.NLADefault","text":"NLADefault()\n\nReturns the array untouched, default option.\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.NLAT1","page":"States Modifications","title":"ReservoirComputing.NLAT1","text":"NLAT1()\n\nApplies the $ \\text{T}_1 $ transformation algorithm, as defined in [1] and [2].\n\n[1] Chattopadhyay, Ashesh, et al. \"Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods:  Reservoir computing, ANN, and RNN-LSTM.\" (2019).\n\n[2] Pathak, Jaideep, et al. \"Model-free prediction of large spatiotemporally chaotic systems from data:  A reservoir computing approach.\" Physical review letters 120.2 (2018): 024102.\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.NLAT2","page":"States Modifications","title":"ReservoirComputing.NLAT2","text":"NLAT2()\n\nApply the $ \\text{T}_2 $ transformation algorithm, as defined in [1].\n\n[1] Chattopadhyay, Ashesh, et al. \"Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods:  Reservoir computing, ANN, and RNN-LSTM.\" (2019).\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.NLAT3","page":"States Modifications","title":"ReservoirComputing.NLAT3","text":"NLAT3()\n\nApply the $ \\text{T}_3 $ transformation algorithm, as defined in [1].\n\n[1] Chattopadhyay, Ashesh, et al. \"Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods:  Reservoir computing, ANN, and RNN-LSTM.\" (2019).\n\n\n\n\n\n","category":"type"},{"location":"esn_tutorials/states_variation/#Altering-ESN-States","page":"Altering ESN States","title":"Altering ESN States","text":"","category":"section"},{"location":"api/esn_drivers/#ESN-Drivers","page":"ESN Drivers","title":"ESN Drivers","text":"","category":"section"},{"location":"api/esn_drivers/","page":"ESN Drivers","title":"ESN Drivers","text":"    RNN\n    MRNN\n    GRU","category":"page"},{"location":"api/esn_drivers/#ReservoirComputing.RNN","page":"ESN Drivers","title":"ReservoirComputing.RNN","text":"RNN(activation_function, leaky_coefficient)\nRNN(;activation_function=tanh, leaky_coefficient=1.0)\n\nReturns a Recurrent Neural Network initializer for the ESN. This is the default choice.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/#ReservoirComputing.MRNN","page":"ESN Drivers","title":"ReservoirComputing.MRNN","text":"MRNN(activation_function, leaky_coefficient, scaling_factor)\nMRNN(;activation_function=[tanh, sigmoid], leaky_coefficient=1.0, scaling_factor=fill(leaky_coefficient, length(activation_function)))\n\nReturns a Multiple RNN initializer, where multiple function are combined in a linear combination with chosen parameters scaling_factor. The activation_function and scaling_factor arguments must vectors of the same size. Multiple combinations are possible,  the implementation is based upon a double activation function idea, found in [1].\n\n[1] Lun, Shu-Xian, et al. \"A novel model of leaky integrator echo state network for time-series prediction.\" Neurocomputing 159 (2015): 58-66.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/#ReservoirComputing.GRU","page":"ESN Drivers","title":"ReservoirComputing.GRU","text":"GRU(;activation_function=[NNlib.sigmoid, NNlib.sigmoid, tanh],\n    layer_init = fill(DenseLayer(), 5),\n    reservoir_init = fill(RandSparseReservoir(), 2),\n    variant = FullyGated())\n\nReturns a Gated Recurrent Unit [1] reservoir driver.\n\n[1] Cho, Kyunghyun, et al. âLearning phrase representations using RNN encoder-decoder for statistical machine translation.â arXiv preprint arXiv:1406.1078 (2014).\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/","page":"ESN Drivers","title":"ESN Drivers","text":"The GRU driver also provides the user the choice of the possible variant:","category":"page"},{"location":"api/esn_drivers/","page":"ESN Drivers","title":"ESN Drivers","text":"    FullyGated\n    Variant1\n    Variant2\n    Variant3\n    Minimal","category":"page"},{"location":"api/esn_drivers/#ReservoirComputing.FullyGated","page":"ESN Drivers","title":"ReservoirComputing.FullyGated","text":"FullyGated()\n\nReturns a standard Gated Recurrent Unit ESN initializer, as described in [1].\n\n[1] Cho, Kyunghyun, et al. âLearning phrase representations using RNN encoder-decoder for statistical machine translation.â  arXiv preprint arXiv:1406.1078 (2014).\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/#ReservoirComputing.Variant1","page":"ESN Drivers","title":"ReservoirComputing.Variant1","text":"Variant1()\n\nReturns the variant 1 ESN initializer of the GRU as described in [1], where each gate is computed using only the prevoius hidden states  and the bias\n\n[1] Dey, Rahul, and Fathi M. Salem. \"Gate-variants of gated recurrent unit (GRU) neural networks.\"  2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS). IEEE, 2017.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/#ReservoirComputing.Variant2","page":"ESN Drivers","title":"ReservoirComputing.Variant2","text":"Variant2()\n\nReturns the variant 2 ESN initializer of the GRU as described in [1], where each gate is computed using only the previous hidden state.\n\n[1] Dey, Rahul, and Fathi M. Salem. \"Gate-variants of gated recurrent unit (GRU) neural networks.\"  2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS). IEEE, 2017.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/#ReservoirComputing.Variant3","page":"ESN Drivers","title":"ReservoirComputing.Variant3","text":"Variant3()\n\nReturns the variant 3 ESN initializer of the GRU as described in [1], where each gate is computed only using the bias.\n\n[1] Dey, Rahul, and Fathi M. Salem. \"Gate-variants of gated recurrent unit (GRU) neural networks.\"  2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS). IEEE, 2017.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/#ReservoirComputing.Minimal","page":"ESN Drivers","title":"ReservoirComputing.Minimal","text":"Minimal()\n\nReturns a minimal GRU ESN initializer as described in [1].\n\n[1] Zhou, Guo-Bing, et al. \"Minimal gated unit for recurrent neural networks.\"  International Journal of Automation and Computing 13.3 (2016): 226-234.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/","page":"ESN Drivers","title":"ESN Drivers","text":"Please refer to the original papers for more detail about these architectures.","category":"page"},{"location":"api/esn_drivers/","page":"ESN Drivers","title":"ESN Drivers","text":"The states are created using the following function","category":"page"},{"location":"api/esn_drivers/","page":"ESN Drivers","title":"ESN Drivers","text":"    create_states","category":"page"},{"location":"api/esn_drivers/#ReservoirComputing.create_states","page":"ESN Drivers","title":"ReservoirComputing.create_states","text":"create_states(reservoir_driver::AbstractReservoirDriver, train_data, reservoir_matrix, input_matrix)\n\nReturn the trained ESN states according to the given driver.\n\n\n\n\n\n","category":"function"},{"location":"api/training/#Training-Algorithms","page":"Training Algorithms","title":"Training Algorithms","text":"","category":"section"},{"location":"api/training/#Linear-Models","page":"Training Algorithms","title":"Linear Models","text":"","category":"section"},{"location":"api/training/","page":"Training Algorithms","title":"Training Algorithms","text":"    StandardRidge\n    LinearModel","category":"page"},{"location":"api/training/#ReservoirComputing.StandardRidge","page":"Training Algorithms","title":"ReservoirComputing.StandardRidge","text":"StandardRidge(regularization_coeff)\nStandardRidge(;regularization_coeff=0.0)\n\nRidge regression training for all the models in the library. The regularization_coeff is the regularization,  it can be passed as an arg or kwarg.\n\n\n\n\n\n","category":"type"},{"location":"api/training/#ReservoirComputing.LinearModel","page":"Training Algorithms","title":"ReservoirComputing.LinearModel","text":"LinearModel(;regression=LinearRegression, \n    solver=Analytical(), \n    regression_kwargs=(;))\n\nLinear regression training based on MLJLinearModels for all the models in the library.  All the parameters have to be passed into regression_kwargs, apart from the solver choice. MLJLinearModels.jl needs to be called in order  to use these models.\n\n\n\n\n\n","category":"type"},{"location":"api/training/#Gaussian-Regression","page":"Training Algorithms","title":"Gaussian Regression","text":"","category":"section"},{"location":"api/training/","page":"Training Algorithms","title":"Training Algorithms","text":"    GaussianProcess","category":"page"},{"location":"api/training/#ReservoirComputing.GaussianProcess","page":"Training Algorithms","title":"ReservoirComputing.GaussianProcess","text":"GaussianProcess(mean, kernel;\n    lognoise=-2, \n    optimize=false,\n    optimizer=Optim.LBFGS())\n\nWrapper around GaussianProcesses gives the possibility of  training every model in the library using Gaussian Regression. GaussianProcesses.jl needs to be called in order to use these  models. The use of Gaussian Regression for ESNs has first been explored in [1].\n\n[1] Chatzis, Sotirios P., and Yiannis Demiris. \"Echo state Gaussian process.\" IEEE Transactions on Neural Networks 22.9 (2011): 1435-1445.\n\n\n\n\n\n","category":"type"},{"location":"api/training/#Support-Vector-Regression","page":"Training Algorithms","title":"Support Vector Regression","text":"","category":"section"},{"location":"api/training/","page":"Training Algorithms","title":"Training Algorithms","text":"Support vector Regression is possible using a direct call to LIBSVM regression methods. Instead of a wrapper please refer to the use of LIBSVM.AbstractSVR in the original library.","category":"page"},{"location":"api/esn_layers/#ESN-Layers","page":"ESN Layers","title":"ESN Layers","text":"","category":"section"},{"location":"api/esn_layers/#Input-Layers","page":"ESN Layers","title":"Input Layers","text":"","category":"section"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"    WeightedLayer\n    DenseLayer\n    SparseLayer\n    InformedLayer\n    MinimumLayer","category":"page"},{"location":"api/esn_layers/#ReservoirComputing.WeightedLayer","page":"ESN Layers","title":"ReservoirComputing.WeightedLayer","text":"WeightedInput(scaling)\nWeightedInput(;scaling=0.1)\n\nReturns a weighted layer initializer object, that will produce a weighted input matrix with  a with random non-zero elements drawn from [-scaling, scaling], as described in [1].  The scaling factor can be given as arg or kwarg.\n\n[1] Lu, Zhixin, et al. \"Reservoir observers: Model-free inference of unmeasured variables in chaotic  systems.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 27.4 (2017): 041102.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/#ReservoirComputing.DenseLayer","page":"ESN Layers","title":"ReservoirComputing.DenseLayer","text":"DenseLayer(scaling)\nDenseLayer(;scaling=0.1)\n\nReturns a fully connected layer initializer object, that will produce a weighted input matrix with  a with random non-zero elements drawn from [-scaling, scaling]. The scaling  factor can be given as arg or kwarg. This is the default choice in the ESN construction.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/#ReservoirComputing.SparseLayer","page":"ESN Layers","title":"ReservoirComputing.SparseLayer","text":"SparseLayer(scaling, sparsity)\nSparseLayer(scaling; sparsity=0.1)\nSparseLayer(;scaling=0.1, sparsity=0.1)\n\nReturns a sparsely connected layer initializer object, that will produce a random sparse  input matrix with random non-zero elements drawn from [-scaling, scaling] and  given sparsity. The scaling and sparsity factors can be given as args or kwargs.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/#ReservoirComputing.InformedLayer","page":"ESN Layers","title":"ReservoirComputing.InformedLayer","text":"InformedLayer(model_in_size; scaling=0.1, gamma=0.5)\n\nReturns a weighted input layer matrix, with random non-zero elements drawn from [-scaling, scaling], where some Î³ of reservoir nodes are connected exclusively to the raw inputs, and the rest to the outputs of the prior knowledge model , as described in [1].\n\n[1] Jaideep Pathak et al. \"Hybrid Forecasting of Chaotic Processes: Using Machine Learning in Conjunction with a Knowledge-Based Model\" (2018)\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/#ReservoirComputing.MinimumLayer","page":"ESN Layers","title":"ReservoirComputing.MinimumLayer","text":"MinimumLayer(weight, sampling)\nMinimumLayer(weight; sampling=BernoulliSample(0.5))\nMinimumLayer(;weight=0.1, sampling=BernoulliSample(0.5))\n\nReturns a fully connected layer initializer object. The matrix constructed with this initializer  presents the same absolute weight value, decided by the weight factor. The sign of each entry is  decided by the sampling struct. Construction detailed in [1] and [2].\n\n[1] Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on  neural networks 22.1 (2010): 131-144. [2] Rodan, Ali, and Peter TiÅo. \"Simple deterministically constructed cycle reservoirs with regular  jumps.\" Neural computation 24.7 (2012): 1822-1852.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"The sign in the MinimumLayer are chosen based on the following methods:","category":"page"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"    BernoulliSample\n    IrrationalSample","category":"page"},{"location":"api/esn_layers/#ReservoirComputing.BernoulliSample","page":"ESN Layers","title":"ReservoirComputing.BernoulliSample","text":"BernoulliSample(p)\nBernoulliSample(;p=0.5)\n\nReturns a Bernoulli sign constructor for the MinimumLayer call. The p factor determines the  probability of the result as in the Distributions call. The value can be passed as an arg or kwarg. This sign weight determination for input layers is introduced in [1]\n\n[1] Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on  neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/#ReservoirComputing.IrrationalSample","page":"ESN Layers","title":"ReservoirComputing.IrrationalSample","text":"IrrationalSample(irrational, start)\nIrrationalSample(;irrational=pi, start=1)\n\nReturns an irrational sign contructor for the '''MinimumLayer''' call. The values can be passed as args or  kwargs. The sign of the weight are decided from the decimal expansion of the given irrational. The  first start decimal digits are thresholded at 4.5, then the n-th input sign will be \n\nand - respectively.\n\n[1] Rodan, Ali, and Peter TiÅo. \"Simple deterministically constructed cycle reservoirs with regular  jumps.\" Neural computation 24.7 (2012): 1822-1852.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"To derive the matrix one can call the following function:","category":"page"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"    create_layer","category":"page"},{"location":"api/esn_layers/#ReservoirComputing.create_layer","page":"ESN Layers","title":"ReservoirComputing.create_layer","text":"create_layer(input_layer::AbstractLayer, res_size, in_size)\n\nReturns a res_size times in_size matrix layer, built accordingly to the input_layer  constructor.\n\n\n\n\n\n","category":"function"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"To create new input layers it suffice to define a new struct containing the needed parameters of the new input layer. This struct wiil need to be an AbstractLayer, so the create_layer function can be dispatched over it. The workflow should follow this snippet:","category":"page"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"#creation of the new struct for the layer\nstruct MyNewLayer <: AbstractLayer\n    #the layer params go here\nend\n\n#dispatch over the function to actually build the layer matrix\nfunction create_layer(input_layer::MyNewLayer, res_size, in_size)\n    #the new algorithm to build the input layer goes here\nend","category":"page"},{"location":"api/esn_layers/#Reservoirs","page":"ESN Layers","title":"Reservoirs","text":"","category":"section"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"    RandSparseReservoir\n    PseudoSVDReservoir\n    DelayLineReservoir\n    DelayLineBackwardReservoir\n    SimpleCycleReservoir\n    CycleJumpsReservoir","category":"page"},{"location":"api/esn_layers/#ReservoirComputing.RandSparseReservoir","page":"ESN Layers","title":"ReservoirComputing.RandSparseReservoir","text":"RandSparseReservoir(radius, sparsity)\nRandSparseReservoir(;radius=1.0, sparsity=0.1)\n\nReturns a random sparse reservoir initializer, that will return a matrix with given sparsity and  scaled spectral radius according to radius. This is the default choice in the ESN construction.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/#ReservoirComputing.PseudoSVDReservoir","page":"ESN Layers","title":"ReservoirComputing.PseudoSVDReservoir","text":"PseudoSVDReservoir(max_value, sparsity, sorted, reverse_sort)\nPseudoSVDReservoir(max_value, sparsity; sorted=true, reverse_sort=false)\n\nReturns an initializer to build a sparse reservoir matrix, with given sparsity created using SVD as described in [1]. \n\n[1] Yang, Cuili, et al. \"Design of polynomial echo state networks for time series prediction.\" Neurocomputing 290 (2018): 148-160.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/#ReservoirComputing.DelayLineReservoir","page":"ESN Layers","title":"ReservoirComputing.DelayLineReservoir","text":"DelayLineReservoir(weight)\nDelayLineReservoir(;weight=0.1)\n\nReturns a Delay Line Reservoir matrix constructor to obtain a deterministi reservoir as described in [1]. The  weight can be passed as arg or kwarg and it determines the absolute value of all the connections in the reservoir.\n\n[1] Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/#ReservoirComputing.DelayLineBackwardReservoir","page":"ESN Layers","title":"ReservoirComputing.DelayLineBackwardReservoir","text":"DelayLineBackwardReservoir(weight, fb_weight)\nDelayLineBackwardReservoir(;weight=0.1, fb_weight=0.2)\n\nReturns a Delay Line Reservoir constructor to create a matrix with Backward connections as described in [1]. The  weight and fb_weight can be passed as either args or kwargs, and they determine the only absolute values of the connections in the reservoir.\n\n[1] Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/#ReservoirComputing.SimpleCycleReservoir","page":"ESN Layers","title":"ReservoirComputing.SimpleCycleReservoir","text":"SimpleCycleReservoir(weight)\nSimpleCycleReservoir(;weight=0.1)\n\nReturns a Simple Cycle Reservoir Reservoir constructor to biuld a reservoir matrix as described in [1]. The  weight can be passed as arg or kwarg and it determines the absolute value of all the connections in the reservoir.\n\n[1] Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/#ReservoirComputing.CycleJumpsReservoir","page":"ESN Layers","title":"ReservoirComputing.CycleJumpsReservoir","text":"CycleJumpsReservoir(;cycle_weight=0.1, jump_weight=0.1, jump_size=3)\nCycleJumpsReservoir(cycle_weight, jump_weight, jump_size)\n\nReturn a Cycle Reservoir with Jumps constructor to create a reservoir matrix as described in [1]. The  weight and jump_weight can be passed as args or kwargs and they determine the absolute values of  all the connections in the reservoir. The jump_size can also be passed either as arg and kwarg and it  detemines the jumps between jump_weights.\n\n[1] Rodan, Ali, and Peter TiÅo. \"Simple deterministically constructed cycle reservoirs with regular jumps.\" Neural computation 24.7 (2012): 1822-1852.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"Like for the input layers, to actually build the matrix of the reservoir one can call the following function:","category":"page"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"    create_reservoir","category":"page"},{"location":"api/esn_layers/#ReservoirComputing.create_reservoir","page":"ESN Layers","title":"ReservoirComputing.create_reservoir","text":"create_reservoir(reservoir::AbstractReservoir, res_size)\n\nGiven an ``AbstractReservoir constructor and the reservoir size it returns the corresponding matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"To create a new reservoir the procedure is imilar to the one for the input layers. First the definition of the new struct of type AbstractReservoir with the reservoir parameters is needed. Then the dispatch over the create_reservoir function makes the model actually build the reservoir matrix. An example of the workflow is given in the following snippet:","category":"page"},{"location":"api/esn_layers/","page":"ESN Layers","title":"ESN Layers","text":"#creation of the new struct for the reservoir\nstruct MyNewReservoir <: AbstractReservoir\n    #the reservoir params go here\nend\n\n#dispatch over the function to build the reservoir matrix\nfunction create_reservoir(reservoir::AbstractReservoir, res_size)\n    #the new algorithm to build the reservoir matrix goes here\nend","category":"page"},{"location":"esn_tutorials/dafesn/#Multiple-Activation-Function-ESN","page":"Multiple Activation Function ESN","title":"Multiple Activation Function ESN","text":"","category":"section"},{"location":"#ReservoirComputing.jl","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"ReservoirComputing.jl provides an efficient, modular and easy to use implementation of Reservoir Computing models such as Echo State Networks (ESNs). Reservoir Computing (RC) is an umbrella term used to describe a family of models such as ESNs and Liquid State Machines (LSMs). The key concept is to expand the input data into a higher dimension and use regression in order to train the model; in some ways Reservoir Computers can be considered similar to kernel methods. ","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"info: Introductory material\nThis library assumes some basic knowledge of Reservoir Computing. For a good introduction, we suggest the following papers: the first two are the seminal papers about ESN and LSM, the others are in-depth review papers that should cover all the needed information. For the majority of the algorithms implemented in this library we cited in the documentation the original work introducing them. If you ever are in doubt about about a method or a function just type ? function in the Julia REPL to read the relevant notes.Jaeger, Herbert: The âecho stateâ approach to analysing and training recurrent neural networks-with an erratum note.\nMaass W, NatschlÃ¤ger T, Markram H: Real-time computing without stable states: a new framework for neural computation based on perturbations.\nLukoÅ¡eviÄius, Mantas: A practical guide to applying echo state networks.\" Neural networks: Tricks of the trade.\nLukoÅ¡eviÄius, Mantas, and Herbert Jaeger: Reservoir computing approaches to recurrent neural network training.","category":"page"},{"location":"#Installation","page":"ReservoirComputing.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"ReservoirComputing.jl is registered in the General Julia Registry, so the installation of the package follows the usual procedure:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"import Pkg; Pkg.add(\"ReservoirComputing\")","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"The support for this library is for Julia v1.6 or greater.","category":"page"},{"location":"#Features-Overview","page":"ReservoirComputing.jl","title":"Features Overview","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"This library provides multiple ways of training the chosen RC model. More specifically the available algorithms are:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"StandardRidge: a naive implementation of Ridge Regression. The default choice for training.\nLinearModel: a wrap around MLJLinearModels.\nGaussianProcess: a wrap around GaussianProcesses.\nLIBSVM.AbstractSVR: a direct call of LIBSVM regression methods.","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Also provided are two different ways of doing predictions using RC:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Generative: the algorithm uses the prediction of the model in the previous step to continue the prediction. It only needs the number of steps as input.\nPredictive: standard Machine Learning type of prediction. Given the features the RC model will return the label/prediction.","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"It is possible to modify the RC obtained states in the training and prediction step using the following:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"StandardStates: default choice, no changes will be made to the states.\nExtendedStates: the states are extended using a vertical concatenation with the input data.\nPaddedStates: the states are padded using a vertical concatenation with the chosing padding value\nPaddedExtendedStates: a combination of the first two. First the states are extended and then padded.","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"In addition another modification is possible through the choice of non linear algorithms:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"NLADefault: default choice, no changes will be made to the states.\nNLAT1\nNLAT2\nNLAT3","category":"page"},{"location":"#Echo-State-Networks","page":"ReservoirComputing.jl","title":"Echo State Networks","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Regarding ESNs in the library are implemented the following input layers:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"WeightedLayer: weighted layer matrix with weights sampled from a uniform distribution.\nDenseLayer: dense layer matrix with weights sampled from a uniform distribution.\nSparseLayer: sparse layer matrix with weights sampled from a uniform distribution.\nMinimumLayer: matrix with constant weights and weight sign decided following one of the two:\nBernoulliSample\nIrrationalSample\nInformedLayer: special kin of weighted layer matrix for Hybrid ESNs.","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"The package also contains multiple implementation of Reservoirs:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"RandSparseReservoir: random sparse matrix with scaling of spectral radius\nPseudoSVDReservoir: Pseudo SVD construction of a random sparse matrix\nDelayLineReservoir: minimal matrix with chosen weights\nDelayLineBackwardReservoir: minimal matrix with chosen weights\nSimpleCycleReservoir: minimal matrix with chosen weights\nCycleJumpsReservoir: minimal matrix with chosen weights","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"In addition multiple ways of driving the reservoir states are also provided:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"RNN: standard Recurrent Neural Network driver.\nMRNN: Multiple RNN driver, it consists on a linear combination of RNNs\nGRU: gated Recurrent Unit driver, with all the possible GRU variants available:\nFullyGated\nVariant1\nVariant2\nVariant3\nMinimal","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"An hybrid version of the model is also available through Hybrid","category":"page"},{"location":"#Reservoir-Computing-with-Cellular-Automata","page":"ReservoirComputing.jl","title":"Reservoir Computing with Cellular Automata","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"The package provides also an inplementation of Reservoir Computing models based on one dimensional Cellular Automata through the RECA call. For the moment the only input encoding available (an input encoding plays a similar role to the input matrix for ESNs) is a random mapping, called through RandomMapping. ","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"All the training methods described above can be used, as well as all the modifications to the states. Both prediction methods are also possible in theory, although in the literature only Predictive tasks have been explored.","category":"page"},{"location":"#Contributing","page":"ReservoirComputing.jl","title":"Contributing","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Contributions are very welcomed! Some interesting variation of RC models are posted in the issues, but everyone is free to just post relevant papers that could fit the scope of the library. Help with the documentation, providing new examples or application cases is also really important and appreciated. Everything that can make the package a little better is a great contribution, no matter how small. The API section of the documentation provides a more in depth look into how things work and are connected, so that is a good place to start exploring more the library. For every doubt that cannot be expressed in issues please feel free to contact any of the lead developers on Slack or by email.","category":"page"},{"location":"api/esn/#Echo-State-Networks","page":"Echo State Networks","title":"Echo State Networks","text":"","category":"section"},{"location":"api/esn/","page":"Echo State Networks","title":"Echo State Networks","text":"    ESN","category":"page"},{"location":"api/esn/#ReservoirComputing.ESN","page":"Echo State Networks","title":"ReservoirComputing.ESN","text":"ESN(input_res_size, train_data;\n    variation = Default(),\n    input_init = DenseLayer(),\n    reservoir_init = RandSparseReservoir(),\n    reservoir_driver = RNN(),\n    nla_type = NLADefault(),\n    states_type = StandardStates())\n(esn::ESN)(prediction::AbstractPrediction,\n    output_layer::AbstractOutputLayer;\n    initial_conditions=output_layer.last_value,\n    last_state=esn.states[:, end])\n\nConstructor for the Echo State Network model. It requires the erserovir size as the input and the data for the training.  It returns a struct ready to be trained with the states already harvested. \n\nAfter the training this struct can be used for the prediction following the second function call. This will take as input a  prediction type and the output layer from the training. The initial_conditions and last_state parameters  can be left as they are, unless there is a specific reason to change them. All the components are detailed in the  API documentation and show how to leverage in the examples.\n\n\n\n\n\n","category":"type"},{"location":"api/esn/","page":"Echo State Networks","title":"Echo State Networks","text":"In addition to all the components that can be explored in the documentation a couple need a separate introduction. The variation arguments can be","category":"page"},{"location":"api/esn/","page":"Echo State Networks","title":"Echo State Networks","text":"    Default\n    Hybrid","category":"page"},{"location":"api/esn/#ReservoirComputing.Default","page":"Echo State Networks","title":"ReservoirComputing.Default","text":"Default()\n\nSets the type of the ESN as the standard model. No parameters are needed.\n\n\n\n\n\n","category":"type"},{"location":"api/esn/#ReservoirComputing.Hybrid","page":"Echo State Networks","title":"ReservoirComputing.Hybrid","text":"Hybrid(prior_model, u0, tspan, datasize)\n\nGiven the model parameters returns an Hybrid variation of the ESN. This entails a different training  and prediction. Construction based on [1].\n\n[1] Jaideep Pathak et al. \"Hybrid Forecasting of Chaotic Processes: Using Machine Learning in Conjunction with a Knowledge-Based Model\" (2018)\n\n\n\n\n\n","category":"type"},{"location":"api/esn/","page":"Echo State Networks","title":"Echo State Networks","text":"These arguments detail more deep variation of the underlying model and they need a separate call. For the moment the most complex is the Hybrid call, but this can and will change in the future. All ESN models can be trained using the following call:","category":"page"},{"location":"api/esn/","page":"Echo State Networks","title":"Echo State Networks","text":"    train","category":"page"},{"location":"api/esn/#ReservoirComputing.train","page":"Echo State Networks","title":"ReservoirComputing.train","text":"train(esn::AbstractEchoStateNetwork, target_data, training_method=StandardRidge(0.0))\n\nTraining of the built ESN over the target_data. The default training method is RidgeRegression. The output is  an OutputLayer object to be fed at the esn call for the prediction.\n\n\n\n\n\n","category":"function"}]
}
